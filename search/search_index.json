{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#roboflow-inference","title":"Roboflow Inference \ud83d\udcbb","text":"<p>Roboflow Inference is an opinionated tool for running inference on state-of-the-art computer vision models. With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments.</p> <p>By using Roboflow Inference, you can:</p> <ol> <li>Write inference logic without having to configure environments or install dependencies.</li> <li>Deploy models to a range of devices and environments.</li> <li>Use state-of-the-art models with your own weights.</li> <li>Query models with HTTP.</li> <li>Scale up your inference server as your production needs grow with Docker.</li> </ol> <p>You can use Inference wherever you can use Docker: on CPUs, GPUs, TRT and more.</p> <p>You can run inference using the <code>inference</code> pip package, or using the HTTP server available by running a Roboflow Inference Docker containers.</p>"},{"location":"#installation","title":"Installation \ud83d\udee0\ufe0f","text":"<p>The Roboflow Inference runs in a Docker container. If you do not already have Docker installed on your computer, follow the official Docker installation instructions to install Docker.</p> <p>For all installation options, you will need a free Roboflow account and a Roboflow API key. You can learn how to retrieve your API key in the Roboflow API documentation.</p>"},{"location":"#install-using-pip","title":"Install Using Pip","text":"<p>To install the Inference using pip, run one of the following commands, depending on the device on which you want to run the Inference:</p> <pre><code>pip install inference[arm]\npip install inference[jetson]\npip install inference[trt]\n</code></pre>"},{"location":"#install-using-docker","title":"Install Using Docker","text":"<p>To install the Inference, first clone this repository:</p> <pre><code>git clone https://github.com/roboflow/inference\n</code></pre> <p>Then, choose a Dockerfile from the options below, depending on the environment in which you want to run the Inference:</p> <ul> <li><code>Dockerfile.onnx.arm.cpu</code>: ARM CPU</li> <li><code>Dockerfile.onnx.cpu</code>: CPU</li> <li><code>Dockerfile.onnx.gpu</code>: GPU</li> <li><code>Dockerfile.onnx.jetson</code>: NVIDIA Jetson</li> <li><code>Dockerfile.onnx.jetson.4.4.1</code>: NVIDIA Jetson 4.4.1</li> <li><code>Dockerfile.onnx.jetson.5.1.1</code>: NVIDIA Jetson 5.1.1</li> <li><code>Dockerfile.onnx.lambda</code>: AWS Lambda</li> <li><code>Dockerfile.onnx.trt</code>: TensorRT</li> </ul> <p>To install the server, first download and build the container:</p> <pre><code>docker build -f dockerfiles/[FILE_NAME] -t inference-server .\n</code></pre> <p>Replace <code>[FILE_NAME]</code> with the name of the Dockerfile that applies to the environment in which you want to run the Inference.</p> <p>Then, run the container:</p> <pre><code>docker run --gpus all --network=host -e PORT=9000 -e API_KEY=&lt;YOUR API KEY&gt; -v $(pwd)/cache:/cache inference-server\n</code></pre> <p>Substitute the API_KEY variable with your Roboflow API key. Learn how to retrieve your Roboflow API key.</p>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":"<p>API documentation are hosted by a running instance of the inference server at the <code>/docs</code> and <code>/redoc</code> endpoints. For an inference server running locally, see <code>https://localhost/docs</code> or <code>https://localhost/redoc</code>.</p>"},{"location":"#model-support","title":"Model Support \ud83d\uddbc\ufe0f","text":"<p>The Roboflow Inference supports the models listed below.</p> <p>You can also run any model hosted on Roboflow using the Inference.</p>"},{"location":"#classification","title":"Classification","text":"<ul> <li>ViT</li> <li>YOLOv8</li> <li>CLIP</li> </ul>"},{"location":"#object-detection","title":"Object Detection","text":"<ul> <li>YOLOv5</li> <li>YOLOv8</li> </ul>"},{"location":"#segmentation","title":"Segmentation","text":"<ul> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOACT</li> <li>Segment Anything</li> </ul>"},{"location":"#environment-variable-control","title":"Environment Variable Control \ud83c\udf10","text":"<p>Use these environment variables to control pingback and to Roboflow as well as other features; if you are running a Docker container, pass these into the docker run command.</p> ENV Variable Description PINGBACK_ENABLED Default is true; if set to the string \"false\", pingback messages are not sent back to Roboflow. PINGBACK_URL Default is <code>https://api.roboflow.com/pingback</code> PINGBACK_INTERVAL_SECONDS Frequency of sending pingback messages, default is 3600 seconds ROBOFLOW_SERVER_UUID If this is set, the ID of the process reported back to Roboflow's UI is the value of this environment variable. Omitting this causes the process (docker container) to generate a new UUID. DEVICE_ID This predates Pingback; if left unset, its default value is \"sample-device-id\" ENABLE_PROMETHEUS if set to any value, this will cause a /metrics endpoint to be created with some FastAPI metrics for Prometheus to scrape; not applicable to the lambda inference server"},{"location":"#community-resources","title":"Community Resources \ud83d\udcda","text":"<ul> <li>Roboflow Inference Documentation</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>[ ] Add support for more models</li> </ul>"},{"location":"#contributing","title":"Contributing \u2328\ufe0f","text":"<p>Thank you for your interest in contributing to the Roboflow Inference! You can learn more about how to start contributing in our contributing guide.</p>"},{"location":"#license","title":"License \ud83d\udcdd","text":"<p>The Roboflow Inference code, enclosed within <code>inference/core</code>, as well as the documentation in <code>docs/</code>, is licnsed under an Apache 2.0 license.</p> <p>The following models, accessible through, Roboflow Inference comes with their own licenses:</p> <ul> <li><code>inference/models/clip</code>: MIT.</li> <li><code>inference/models/sam</code>: Apache 2.0.</li> <li><code>inference/models/yolact</code>: MIT.</li> <li><code>inference/models/yolov5</code>: AGPL-3.0.</li> <li><code>inference/models/yolov7</code>: GPL-3.0.</li> <li><code>inference/models/yolov8</code>: AGPL-3.0.</li> </ul> <p>Roboflow Inference offers more features to Enterprise License holders, including:</p> <ol> <li>Running Inference on a cluster of multiple servers</li> <li>Handling auto-batched inference</li> <li>Device management</li> <li>Active learning</li> <li>Sub-license YOLOv5 and YOLOv8 models for enterprise use</li> <li>And more.</li> </ol> <p>To learn more about a Roboflow Inference Enterprise License, contact us.</p>"},{"location":"#build-the-documentation","title":"Build the Documentation","text":"<p>Roboflow Inference uses <code>mkdocs</code> and <code>mike</code> to offer versioned documentation. The project documentation is hosted at https://inference.roboflow.com</p> <p>To build the Inference documentation, first install the project development dependencies:</p> <pre><code>pip install -r dev-requirements.txt\n</code></pre> <p>To run the latest version of the documentation, run:</p> <pre><code>mike serve\n</code></pre> <p>Before a new release is published, a new version of the documentation should be built. To create a new version, run:</p> <pre><code>mike deploy &lt;version-number&gt;\n</code></pre>"},{"location":"add_a_model/","title":"Add a Model to the Inference Server","text":"<p>The Roboflow Inference Server comes with several state-of-the-art models available for inference out of the box. If your model is not supported by the Inference Server, you can implement support for a custom model architecture.</p> <p>In this guide, we are going to walk through how to add support for a new model architecture to the Roboflow Inference Server.</p>"},{"location":"api/","title":"API Reference","text":"<p>The Roboflow Inference Server provides OpenAPI documentation at the <code>/docs</code> endpoint for use in development.</p> <p>Below is the OpenAPI specification for the Inference Server, rendered with Swagger.</p> <p></p>"},{"location":"contributing/","title":"Contributing to the Roboflow Inference Server \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to the Roboflow Inference Server!</p> <p>We welcome any contributions to help us improve the quality of <code>inference-server</code> and expand the range of supported models.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add support for running inference on a new model.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>The Inference Server provides a standard interface through which you can work with computer vision models. With Inference Server, you can use state-of-the-art models with your own weights without having to spend time installing dependencies, configuring environments, and writing inference code.</p> <p>We welcome contributions that add support for new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>You will need to add documentation for your model and link to it from the <code>inference-server</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"library/data_models/clip/clip_compare_request/","title":"CLIP Compare Request","text":"<p>         Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>Union[InferenceRequestImage, str]</code> <p>The type of image data provided, one of 'url' or 'base64'.</p> <code>subject_type</code> <code>str</code> <p>The type of subject, one of 'image' or 'text'.</p> <code>prompt</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]</code> <p>The prompt for comparison.</p> <code>prompt_type</code> <code>str</code> <p>The type of prompt, one of 'image' or 'text'.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ClipCompareRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP comparison.\n\n    Attributes:\n        subject (Union[InferenceRequestImage, str]): The type of image data provided, one of 'url' or 'base64'.\n        subject_type (str): The type of subject, one of 'image' or 'text'.\n        prompt (Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]): The prompt for comparison.\n        prompt_type (str): The type of prompt, one of 'image' or 'text'.\n    \"\"\"\n\n    subject: Union[InferenceRequestImage, str] = Field(\n        example=\"url\",\n        description=\"The type of image data provided, one of 'url' or 'base64'\",\n    )\n    subject_type: str = Field(\n        default=\"image\",\n        example=\"image\",\n        description=\"The type of subject, one of 'image' or 'text'\",\n    )\n    prompt: Union[\n        List[InferenceRequestImage],\n        InferenceRequestImage,\n        str,\n        List[str],\n        Dict[str, Union[InferenceRequestImage, str]],\n    ]\n    prompt_type: str = Field(\n        default=\"text\",\n        example=\"text\",\n        description=\"The type of prompt, one of 'image' or 'text'\",\n    )\n</code></pre>"},{"location":"library/data_models/clip/clip_image_embedding_request/","title":"CLIP Image Embedding Request","text":"<p>         Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP image embedding.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be embedded.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ClipImageEmbeddingRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP image embedding.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be embedded.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"library/data_models/clip/clip_inference_request/","title":"CLIP Inference Request","text":"<p>         Bases: <code>BaseModel</code></p> <p>Request for CLIP inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>clip_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ClipInferenceRequest(BaseModel):\n\"\"\"Request for CLIP inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        clip_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    api_key: Optional[str] = ApiKey\n    clip_version_id: Optional[str] = Field(\n        default=CLIP_VERSION_ID,\n        example=\"ViT-B-16\",\n        description=\"The version ID of CLIP to be used for this request. Must be one of RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, ViT-B-32, ViT-L-14-336px, and ViT-L-14.\",\n    )\n</code></pre>"},{"location":"library/data_models/clip/clip_text_embedding_request/","title":"CLIP Text Embedding Request","text":"<p>         Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP text embedding.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Union[List[str], str]</code> <p>A string or list of strings.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ClipTextEmbeddingRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP text embedding.\n\n    Attributes:\n        text (Union[List[str], str]): A string or list of strings.\n    \"\"\"\n\n    text: Union[List[str], str] = Field(\n        example=\"The quick brown fox jumps over the lazy dog\",\n        description=\"A string or list of strings\",\n    )\n</code></pre>"},{"location":"library/data_models/inference/classification/","title":"Classification Inference Request","text":"<p>         Bases: <code>CVInferenceRequest</code></p> <p>Classification inference request.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ClassificationInferenceRequest(CVInferenceRequest):\n\"\"\"Classification inference request.\n\n    Attributes:\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    confidence: Optional[float] = Field(\n        default=0.0,\n        example=0.5,\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        example=1,\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        example=False,\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n</code></pre>"},{"location":"library/data_models/inference/cv_inference_request/","title":"CV Inference Request","text":"<p>         Bases: <code>InferenceRequest</code></p> <p>Computer Vision inference request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class CVInferenceRequest(InferenceRequest):\n\"\"\"Computer Vision inference request.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"library/data_models/inference/inference_request_image/","title":"Inference Request Image","text":"<p>         Bases: <code>BaseModel</code></p> <p>Image data for inference request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of image data provided, one of 'url', 'base64', or 'numpy'.</p> <code>value</code> <code>Optional[Any]</code> <p>Image data corresponding to the image type.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class InferenceRequestImage(BaseModel):\n\"\"\"Image data for inference request.\n\n    Attributes:\n        type (str): The type of image data provided, one of 'url', 'base64', or 'numpy'.\n        value (Optional[Any]): Image data corresponding to the image type.\n    \"\"\"\n\n    type: str = Field(\n        example=\"url\",\n        description=\"The type of image data provided, one of 'url', 'base64', or 'numpy'\",\n    )\n    value: Optional[Any] = Field(\n        example=\"http://www.example-image-url.com\",\n        description=\"Image data corresponding to the image type, if type = 'url' then value is a string containing the url of an image, else if type = 'base64' then value is a string containing base64 encoded image data, else if type = 'numpy' then value is binary numpy data serialized using pickle.dumps(); array should 3 dimensions, channels last, with values in the range [0,255].\",\n    )\n</code></pre>"},{"location":"library/data_models/inference/instance_segmentation/","title":"Instance Segmentation Inference Request","text":"<p>         Bases: <code>ObjectDetectionInferenceRequest</code></p> <p>Instance Segmentation inference request.</p> <p>Attributes:</p> Name Type Description <code>mask_decode_mode</code> <code>Optional[str]</code> <p>The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.</p> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>The amount to tradeoff between 0='fast' and 1='accurate'.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class InstanceSegmentationInferenceRequest(ObjectDetectionInferenceRequest):\n\"\"\"Instance Segmentation inference request.\n\n    Attributes:\n        mask_decode_mode (Optional[str]): The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.\n        tradeoff_factor (Optional[float]): The amount to tradeoff between 0='fast' and 1='accurate'.\n    \"\"\"\n\n    mask_decode_mode: Optional[str] = Field(\n        default=\"accurate\",\n        example=\"accurate\",\n        description=\"The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'\",\n    )\n    tradeoff_factor: Optional[float] = Field(\n        default=0.0,\n        example=0.5,\n        description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n    )\n</code></pre>"},{"location":"library/data_models/inference/object_detection/","title":"Object Detection Inference Request","text":"<p>         Bases: <code>CVInferenceRequest</code></p> <p>Object Detection inference request.</p> <p>Attributes:</p> Name Type Description <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>If true, NMS is applied to all detections at once, if false, NMS is applied per class.</p> <code>class_filter</code> <code>Optional[List[str]]</code> <p>If provided, only predictions for the listed classes will be returned.</p> <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>fix_batch_size</code> <code>Optional[bool]</code> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p> <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold that must be met for a box pair to be considered duplicate during NMS.</p> <code>max_detections</code> <code>Optional[int]</code> <p>The maximum number of detections that will be returned.</p> <code>max_candidates</code> <code>Optional[int]</code> <p>The maximum number of candidate detections passed to NMS.</p> <code>visualization_labels</code> <code>Optional[bool]</code> <p>If true, labels will be rendered on prediction visualizations.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class ObjectDetectionInferenceRequest(CVInferenceRequest):\n\"\"\"Object Detection inference request.\n\n    Attributes:\n        class_agnostic_nms (Optional[bool]): If true, NMS is applied to all detections at once, if false, NMS is applied per class.\n        class_filter (Optional[List[str]]): If provided, only predictions for the listed classes will be returned.\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        fix_batch_size (Optional[bool]): If true, the batch size will be fixed to the maximum batch size configured for this server.\n        iou_threshold (Optional[float]): The IoU threshold that must be met for a box pair to be considered duplicate during NMS.\n        max_detections (Optional[int]): The maximum number of detections that will be returned.\n        max_candidates (Optional[int]): The maximum number of candidate detections passed to NMS.\n        visualization_labels (Optional[bool]): If true, labels will be rendered on prediction visualizations.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    class_agnostic_nms: Optional[bool] = Field(\n        default=False,\n        example=False,\n        description=\"If true, NMS is applied to all detections at once, if false, NMS is applied per class\",\n    )\n    class_filter: Optional[List[str]] = Field(\n        default=None,\n        example=[\"class-1\", \"class-2\", \"class-n\"],\n        description=\"If provided, only predictions for the listed classes will be returned\",\n    )\n    confidence: Optional[float] = Field(\n        default=0.0,\n        example=0.5,\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    fix_batch_size: Optional[bool] = Field(\n        default=False,\n        example=False,\n        description=\"If true, the batch size will be fixed to the maximum batch size configured for this server\",\n    )\n    iou_threshold: Optional[float] = Field(\n        default=1.0,\n        example=0.5,\n        description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n    )\n    max_detections: Optional[int] = Field(\n        default=300,\n        example=300,\n        description=\"The maximum number of detections that will be returned\",\n    )\n    max_candidates: Optional[int] = Field(\n        default=3000,\n        description=\"The maximum number of candidate detections passed to NMS\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        example=False,\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        example=1,\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        example=False,\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n</code></pre>"},{"location":"library/data_models/sam/sam_embedding_request/","title":"SAM Embedding Request","text":"<p>         Bases: <code>SamInferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class SamEmbeddingRequest(SamInferenceRequest):\n\"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        example=\"image_id\",\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        example=\"json\",\n        description=\"The format of the response. Must be one of json or binary. If binary, embedding is returned as a binary numpy array.\",\n    )\n</code></pre>"},{"location":"library/data_models/sam/sam_segmentation_request/","title":"SAM Segmentation Request","text":"<p>         Bases: <code>SamInferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Optional[Union[List[List[List[List[float]]]], Any]]</code> <p>The embeddings to be decoded.</p> <code>embeddings_format</code> <code>Optional[str]</code> <p>The format of the embeddings.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Whether or not the request includes a mask input.</p> <code>mask_input</code> <code>Optional[Union[List[List[List[float]]], Any]]</code> <p>The set of output masks.</p> <code>mask_input_format</code> <code>Optional[str]</code> <p>The format of the mask input.</p> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>The original size of the image used to generate the embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Whether or not to use the mask input cache.</p> Source code in <code>inference/core/data_models.py</code> <pre><code>class SamSegmentationRequest(SamInferenceRequest):\n\"\"\"SAM segmentation request.\n\n    Attributes:\n        embeddings (Optional[Union[List[List[List[List[float]]]], Any]]): The embeddings to be decoded.\n        embeddings_format (Optional[str]): The format of the embeddings.\n        format (Optional[str]): The format of the response.\n        image (Optional[InferenceRequestImage]): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        has_mask_input (Optional[bool]): Whether or not the request includes a mask input.\n        mask_input (Optional[Union[List[List[List[float]]], Any]]): The set of output masks.\n        mask_input_format (Optional[str]): The format of the mask input.\n        orig_im_size (Optional[List[int]]): The original size of the image used to generate the embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n        use_mask_input_cache (Optional[bool]): Whether or not to use the mask input cache.\n    \"\"\"\n\n    embeddings: Optional[Union[List[List[List[List[float]]]], Any]] = Field(\n        example=\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\",\n        description=\"The embeddings to be decoded. The dimensions of the embeddings are 1 x 256 x 64 x 64. If embeddings is not provided, image must be provided.\",\n    )\n    embeddings_format: Optional[str] = Field(\n        default=\"json\",\n        example=\"json\",\n        description=\"The format of the embeddings. Must be one of json or binary. If binary, embeddings are expected to be a binary numpy array.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        example=\"json\",\n        description=\"The format of the response. Must be one of json or binary. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons, then returned as json.\",\n    )\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be segmented. Only required if embeddings are not provided.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        example=\"image_id\",\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    has_mask_input: Optional[bool] = Field(\n        default=False,\n        example=True,\n        description=\"Whether or not the request includes a mask input. If true, the mask input must be provided.\",\n    )\n    mask_input: Optional[Union[List[List[List[float]]], Any]] = Field(\n        default=None,\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256. This is the same as the output, low resolution mask from the previous inference.\",\n    )\n    mask_input_format: Optional[str] = Field(\n        default=\"json\",\n        example=\"json\",\n        description=\"The format of the mask input. Must be one of json or binary. If binary, mask input is expected to be a binary numpy array.\",\n    )\n    orig_im_size: Optional[List[int]] = Field(\n        default=None,\n        example=[640, 320],\n        description=\"The original size of the image used to generate the embeddings. This is only required if the image is not provided.\",\n    )\n    point_coords: Optional[List[List[float]]] = Field(\n        default=[[0.0, 0.0]],\n        example=[[10.0, 10.0]],\n        description=\"The coordinates of the interactive points used during decoding. Each point (x,y pair) corresponds to a label in point_labels.\",\n    )\n    point_labels: Optional[List[float]] = Field(\n        default=[-1],\n        example=[1],\n        description=\"The labels of the interactive points used during decoding. A 1 represents a positive point (part of the object to be segmented). A -1 represents a negative point (not part of the object to be segmented). Each label corresponds to a point in point_coords.\",\n    )\n    use_mask_input_cache: Optional[bool] = Field(\n        default=True,\n        example=True,\n        description=\"Whether or not to use the mask input cache. If true, the mask input cache will be used if it exists. If false, the mask input cache will not be used.\",\n    )\n</code></pre>"},{"location":"library/interfaces/base/","title":"Base Interface","text":"<p>Base interface class which accepts a model manager on initialization</p> Source code in <code>inference/core/interfaces/base.py</code> <pre><code>class BaseInterface:\n\"\"\"Base interface class which accepts a model manager on initialization\"\"\"\n\n    def __init__(self, model_manager: ModelManager) -&gt; None:\n        self.model_manager = model_manager\n</code></pre>"},{"location":"library/interfaces/http/","title":"HTTP Interface","text":"<p>         Bases: <code>BaseInterface</code></p> <p>Roboflow defined HTTP interface for a general-purpose inference server.</p> <p>This class sets up the FastAPI application and adds necessary middleware, as well as initializes the model manager and model registry for the inference server.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry containing the Roboflow models.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>class HttpInterface(BaseInterface):\n\"\"\"Roboflow defined HTTP interface for a general-purpose inference server.\n\n    This class sets up the FastAPI application and adds necessary middleware,\n    as well as initializes the model manager and model registry for the inference server.\n\n    Attributes:\n        app (FastAPI): The FastAPI application instance.\n        model_manager (ModelManager): The manager for handling different models.\n        model_registry (RoboflowModelRegistry): The registry containing the Roboflow models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        model_registry: RoboflowModelRegistry,\n        root_path: Optional[str] = None,\n    ):\n\"\"\"\n        Initializes the HttpInterface with given model manager and model registry.\n\n        Args:\n            model_manager (ModelManager): The manager for handling different models.\n            model_registry (RoboflowModelRegistry): The registry containing the Roboflow models.\n            root_path (Optional[str]): The root path for the FastAPI application.\n\n        Description:\n            Deploy Roboflow trained models to nearly any compute environment!\n        \"\"\"\n        description = \"Roboflow inference server\"\n        app = FastAPI(\n            title=\"Roboflow Inference Server\",\n            description=description,\n            version=__version__,\n            terms_of_service=\"https://roboflow.com/terms\",\n            contact={\n                \"name\": \"Roboflow Inc.\",\n                \"url\": \"https://roboflow.com/contact\",\n                \"email\": \"help@roboflow.com\",\n            },\n            license_info={\n                \"name\": \"Apache 2.0\",\n                \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n            },\n            root_path=root_path,\n        )\n        if METLO_KEY:\n            app.add_middleware(\n                ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n            )\n\n        if len(ALLOW_ORIGINS) &gt; 0:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=ALLOW_ORIGINS,\n                allow_credentials=True,\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n\n        # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n        if PROFILE:\n            app.add_middleware(\n                CProfileMiddleware,\n                enable=True,\n                server_app=app,\n                filename=\"/profile/output.pstats\",\n                strip_dirs=False,\n                sort_by=\"cumulative\",\n            )\n\n        if PINGBACK_ENABLED:\n\n            @app.middleware(\"http\")\n            async def count_errors(request: Request, call_next):\n\"\"\"Middleware to count errors.\n\n                Args:\n                    request (Request): The incoming request.\n                    call_next (Callable): The next middleware or endpoint to call.\n\n                Returns:\n                    Response: The response from the next middleware or endpoint.\n                \"\"\"\n                response = await call_next(request)\n                if response.status_code &gt;= 400:\n                    model_manager.model_manager.num_errors += 1\n                return response\n\n        self.app = app\n        self.model_manager = model_manager\n        self.model_registry = model_registry\n\n        def process_inference_request(\n            inference_request: M.InferenceRequest,\n        ) -&gt; M.InferenceResponse:\n\"\"\"Processes an inference request by calling the appropriate model.\n\n            Args:\n                inference_request (M.InferenceRequest): The request containing model ID and other inference details.\n\n            Returns:\n                M.InferenceResponse: The response containing the inference results.\n            \"\"\"\n            if inference_request.model_id not in self.model_manager:\n                model = self.model_registry.get_model(\n                    inference_request.model_id, inference_request.api_key\n                )(\n                    model_id=inference_request.model_id,\n                    api_key=inference_request.api_key,\n                )\n                self.model_manager.add_model(inference_request.model_id, model)\n            return self.model_manager.infer(\n                inference_request.model_id, inference_request\n            )\n\n        def load_core_model(\n            inference_request: M.InferenceRequest,\n            api_key: Optional[str] = None,\n            core_model: str = None,\n        ) -&gt; None:\n\"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n            Args:\n                inference_request (M.InferenceRequest): The request containing version and other details.\n                api_key (Optional[str]): The API key for the request.\n                core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n            Returns:\n                str: The core model ID.\n            \"\"\"\n            if api_key:\n                inference_request.api_key = api_key\n            version_id_field = f\"{core_model}_version_id\"\n            core_model_id = (\n                f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n            )\n            if core_model_id not in self.model_manager:\n                model = self.model_registry.get_model(\n                    core_model_id, inference_request.api_key\n                )(\n                    model_id=core_model_id,\n                    api_key=inference_request.api_key,\n                )\n                self.model_manager.add_model(core_model_id, model)\n            return core_model_id\n\n        load_clip_model = partial(load_core_model, core_model=\"clip\")\n\"\"\"Loads the CLIP model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The CLIP model ID.\n        \"\"\"\n\n        load_sam_model = partial(load_core_model, core_model=\"sam\")\n\"\"\"Loads the SAM model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The SAM model ID.\n        \"\"\"\n\n        @app.get(\n            \"/\",\n            response_model=M.ServerVersionInfo,\n            summary=\"Root\",\n            description=\"Get the server name and version number\",\n        )\n        async def root():\n\"\"\"Endpoint to get the server name and version number.\n\n            Returns:\n                M.ServerVersionInfo: The server version information.\n            \"\"\"\n            return M.ServerVersionInfo(\n                name=\"Roboflow Inference Server\",\n                version=__version__,\n                uuid=model_manager.model_manager.uuid,\n            )\n\n        # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n        if not LAMBDA:\n\n            @app.get(\n                \"/model/registry\",\n                response_model=M.ModelManagerKeys,\n                summary=\"Get model keys\",\n                description=\"Get the ID of each loaded model\",\n            )\n            async def registry():\n\"\"\"Get the ID of each loaded model in the registry.\n\n                Returns:\n                    M.ModelManagerKeys: The object containing the IDs of all loaded models.\n                \"\"\"\n\n                return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n            @app.post(\n                \"/model/add\",\n                response_model=M.ModelManagerKeys,\n                summary=\"Load a model\",\n                description=\"Load the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_add(request: M.AddModelRequest):\n\"\"\"Load the model with the given model ID into the model manager.\n\n                Args:\n                    request (M.AddModelRequest): The request containing the model ID and optional API key.\n\n                Returns:\n                    M.ModelManagerKeys: The object containing the IDs of all loaded models.\n                \"\"\"\n\n                if request.model_id not in self.model_manager:\n                    model_class = self.model_registry.get_model(\n                        request.model_id, request.api_key\n                    )\n                    model = model_class(\n                        model_id=request.model_id, api_key=request.api_key\n                    )\n                    self.model_manager.add_model(request.model_id, model)\n\n                return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n            @app.post(\n                \"/model/remove\",\n                response_model=M.ModelManagerKeys,\n                summary=\"Remove a model\",\n                description=\"Remove the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_remove(request: M.ClearModelRequest):\n\"\"\"Remove the model with the given model ID from the model manager.\n\n                Args:\n                    request (M.ClearModelRequest): The request containing the model ID to be removed.\n\n                Returns:\n                    M.ModelManagerKeys: The object containing the IDs of all loaded models.\n                \"\"\"\n\n                self.model_manager.remove(request.model_id)\n                return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n            @app.post(\n                \"/model/clear\",\n                response_model=M.ModelManagerKeys,\n                summary=\"Remove all models\",\n                description=\"Remove all loaded models\",\n            )\n            @with_route_exceptions\n            async def model_clear():\n\"\"\"Remove all loaded models from the model manager.\n\n                Returns:\n                    M.ModelManagerKeys: The object containing the IDs of all loaded models (empty set in this case).\n                \"\"\"\n\n                self.model_manager.clear()\n                return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n            @app.post(\n                \"/infer/object_detection\",\n                response_model=Union[\n                    M.ObjectDetectionInferenceResponse,\n                    List[M.ObjectDetectionInferenceResponse],\n                ],\n                summary=\"Object detection infer\",\n                description=\"Run inference with the specified object detection model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def infer_object_detection(\n                inference_request: M.ObjectDetectionInferenceRequest,\n            ):\n\"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (M.ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n\n                Returns:\n                    Union[M.ObjectDetectionInferenceResponse, List[M.ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n\n                return process_inference_request(inference_request)\n\n            @app.post(\n                \"/infer/instance_segmentation\",\n                response_model=M.InstanceSegmentationInferenceResponse,\n                summary=\"Instance segmentation infer\",\n                description=\"Run inference with the specified instance segmentation model\",\n            )\n            @with_route_exceptions\n            async def infer_instance_segmentation(\n                inference_request: M.InstanceSegmentationInferenceRequest,\n            ):\n\"\"\"Run inference with the specified instance segmentation model.\n\n                Args:\n                    inference_request (M.InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n\n                Returns:\n                    M.InstanceSegmentationInferenceResponse: The response containing the inference results.\n                \"\"\"\n\n                return process_inference_request(inference_request)\n\n            @app.post(\n                \"/infer/classification\",\n                response_model=Union[\n                    M.ClassificationInferenceResponse,\n                    M.MultiLabelClassificationInferenceResponse,\n                ],\n                summary=\"Classification infer\",\n                description=\"Run inference with the specified classification model\",\n            )\n            @with_route_exceptions\n            async def infer_classification(\n                inference_request: M.ClassificationInferenceRequest,\n            ):\n\"\"\"Run inference with the specified classification model.\n\n                Args:\n                    inference_request (M.ClassificationInferenceRequest): The request containing the necessary details for classification.\n\n                Returns:\n                    Union[M.ClassificationInferenceResponse, M.MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n\n                return process_inference_request(inference_request)\n\n        if CORE_MODELS_ENABLED:\n            if CORE_MODEL_CLIP_ENABLED:\n\n                @app.post(\n                    \"/clip/embed_image\",\n                    response_model=M.ClipEmbeddingResponse,\n                    summary=\"CLIP Image Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_image(\n                    inference_request: M.ClipImageEmbeddingRequest,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    request: Request = Body(),\n                ):\n\"\"\"\n                    Embeds image data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (M.ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.ClipEmbeddingResponse: The response containing the embedded image.\n                    \"\"\"\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = self.model_manager.infer(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/embed_text\",\n                    response_model=M.ClipEmbeddingResponse,\n                    summary=\"CLIP Text Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed text data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_text(\n                    inference_request: M.ClipTextEmbeddingRequest,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    request: Request = Body(),\n                ):\n\"\"\"\n                    Embeds text data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (M.ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.ClipEmbeddingResponse: The response containing the embedded text.\n                    \"\"\"\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = self.model_manager.infer(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/compare\",\n                    response_model=M.ClipCompareResponse,\n                    summary=\"CLIP Compare\",\n                    description=\"Run the Open AI CLIP model to compute similarity scores.\",\n                )\n                @with_route_exceptions\n                async def clip_compare(\n                    inference_request: M.ClipCompareRequest,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    request: Request = Body(),\n                ):\n\"\"\"\n                    Computes similarity scores using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (M.ClipCompareRequest): The request containing the data to be compared.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.ClipCompareResponse: The response containing the similarity scores.\n                    \"\"\"\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = self.model_manager.infer(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor, n=2)\n                    return response\n\n            if CORE_MODEL_SAM_ENABLED:\n\n                @app.post(\n                    \"/sam/embed_image\",\n                    response_model=M.SamEmbeddingResponse,\n                    summary=\"SAM Image Embeddings\",\n                    description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def sam_embed_image(\n                    inference_request: M.SamEmbeddingRequest,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    request: Request = Body(),\n                ):\n\"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (M.SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                    \"\"\"\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = self.model_manager.infer(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response.embeddings,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n                @app.post(\n                    \"/sam/segment_image\",\n                    response_model=M.SamSegmentationResponse,\n                    summary=\"SAM Image Segmentation\",\n                    description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                async def sam_segment_image(\n                    inference_request: M.SamSegmentationRequest,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    request: Request = Body(),\n                ):\n\"\"\"\n                    Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (M.SamSegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = self.model_manager.infer(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n        if LEGACY_ROUTE_ENABLED:\n            # Legacy object detection inference path for backwards compatability\n            @app.post(\n                \"/{dataset_id}/{version_id}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    M.InstanceSegmentationInferenceResponse,\n                    M.ObjectDetectionInferenceResponse,\n                    M.ClassificationInferenceResponse,\n                    M.MultiLabelClassificationInferenceResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def legacy_infer(\n                dataset_id: str = Path(\n                    description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n                ),\n                version_id: str = Path(\n                    description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n                ),\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                confidence: float = Query(\n                    0.4,\n                    description=\"The confidence threshold used to filter out predictions\",\n                ),\n                format: str = Query(\n                    \"json\",\n                    description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n                ),\n                image: Optional[str] = Query(\n                    None,\n                    description=\"The publically accessible URL of an image to use for inference.\",\n                ),\n                image_type: Optional[str] = Query(\n                    \"base64\",\n                    description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n                ),\n                labels: Optional[bool] = Query(\n                    False,\n                    description=\"If true, labels will be include in any inference visualization.\",\n                ),\n                mask_decode_mode: Optional[str] = Query(\n                    \"accurate\",\n                    description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n                ),\n                tradeoff_factor: Optional[float] = Query(\n                    0.0,\n                    description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n                ),\n                max_detections: int = Query(\n                    300,\n                    description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n                ),\n                overlap: float = Query(\n                    0.3,\n                    description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n                ),\n                request: Request = Body(),\n                stroke: int = Query(\n                    1, description=\"The stroke width used when visualizing predictions\"\n                ),\n                countinference: Optional[bool] = Query(\n                    True,\n                    description=\"If false, does not track inference against usage.\",\n                    include_in_schema=False,\n                ),\n                service_secret: Optional[str] = Query(\n                    None,\n                    description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                    include_in_schema=False,\n                ),\n            ):\n\"\"\"\n                Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n                Args:\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    # Other parameters described in the function signature...\n\n                Returns:\n                    Union[M.InstanceSegmentationInferenceResponse, M.ObjectDetectionInferenceResponse, M.ClassificationInferenceResponse, M.MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n                \"\"\"\n                model_id = f\"{dataset_id}/{version_id}\"\n\n                if confidence &gt;= 1:\n                    confidence /= 100\n                elif confidence &lt; 0.01:\n                    confidence = 0.01\n\n                if overlap &gt;= 1:\n                    overlap /= 100\n\n                if image is not None:\n                    request_image = M.InferenceRequestImage(type=\"url\", value=image)\n                else:\n                    if \"Content-Type\" not in request.headers:\n                        raise ContentTypeMissing(\n                            f\"Request must include a Content-Type header\"\n                        )\n                    if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                        form_data = await request.form()\n                        base64_image_str = form_data[\"file\"].file\n                        request_image = M.InferenceRequestImage(\n                            type=\"multipart\", value=base64_image_str\n                        )\n                    elif (\n                        \"application/x-www-form-urlencoded\"\n                        in request.headers[\"Content-Type\"]\n                        or \"application/json\" in request.headers[\"Content-Type\"]\n                    ):\n                        data = await request.body()\n                        request_image = M.InferenceRequestImage(\n                            type=image_type, value=data\n                        )\n                    else:\n                        raise ContentTypeInvalid(\n                            f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                        )\n\n                if LAMBDA:\n                    request_model_id = (\n                        request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                            \"lambda\"\n                        ][\"model\"][\"endpoint\"]\n                        .replace(\"--\", \"/\")\n                        .replace(\"rf-\", \"\")\n                        .replace(\"nu-\", \"\")\n                    )\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"actor\"]\n                    if countinference:\n                        trackUsage(request_model_id, actor)\n                    else:\n                        if service_secret != ROBOFLOW_SERVICE_SECRET:\n                            raise MissingServiceSecretError(\n                                \"Service secret is required to disable inference usage tracking\"\n                            )\n                else:\n                    request_model_id = model_id\n\n                if request_model_id not in self.model_manager:\n                    model = self.model_registry.get_model(model_id, api_key)(\n                        model_id=request_model_id, api_key=api_key\n                    )\n                    self.model_manager.add_model(request_model_id, model)\n\n                task_type = self.model_manager.get_task_type(request_model_id)\n                inference_request_type = M.ObjectDetectionInferenceRequest\n                args = dict()\n                if task_type == \"instance-segmentation\":\n                    inference_request_type = M.InstanceSegmentationInferenceRequest\n                    args = {\n                        \"mask_decode_mode\": mask_decode_mode,\n                        \"tradeoff_factor\": tradeoff_factor,\n                    }\n                elif task_type == \"classification\":\n                    inference_request_type = M.ClassificationInferenceRequest\n\n                inference_request = inference_request_type(\n                    model_id=request_model_id,\n                    image=request_image,\n                    confidence=confidence,\n                    iou_threshold=overlap,\n                    max_detections=max_detections,\n                    visualization_labels=labels,\n                    visualization_stroke_width=stroke,\n                    visualize_predictions=True if format == \"image\" else False,\n                    **args,\n                )\n\n                inference_response = self.model_manager.infer(\n                    inference_request.model_id, inference_request\n                )\n\n                if format == \"image\":\n                    return Response(\n                        content=inference_response.visualization,\n                        media_type=\"image/jpeg\",\n                    )\n                else:\n                    return inference_response\n\n        if not LAMBDA:\n            # Legacy clear cache endpoint for backwards compatability\n            @app.get(\"/clear_cache\", response_model=str)\n            async def legacy_clear_cache():\n\"\"\"\n                Clears the model cache.\n\n                This endpoint provides a way to clear the cache of loaded models.\n\n                Returns:\n                    str: A string indicating that the cache has been cleared.\n                \"\"\"\n                await model_clear()\n                return \"Cache Cleared\"\n\n            # Legacy add model endpoint for backwards compatability\n            @app.get(\"/start/{dataset_id}/{version_id}\")\n            async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n\"\"\"\n                Starts a model inference session.\n\n                This endpoint initializes and starts an inference session for the specified model version.\n\n                Args:\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                    api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n                Returns:\n                    JSONResponse: A response object containing the status and a success message.\n                \"\"\"\n                model_id = f\"{dataset_id}/{version_id}\"\n                model = self.model_registry.get_model(model_id, api_key)(\n                    model_id=model_id, api_key=api_key\n                )\n                self.model_manager.add_model(model_id, model)\n\n                return JSONResponse(\n                    {\n                        \"status\": 200,\n                        \"message\": \"inference session started from local memory.\",\n                    }\n                )\n\n    def run(self):\n        uvicorn.run(self.app, host=\"127.0.0.1\", port=8080)\n</code></pre>"},{"location":"library/interfaces/http/#inference.core.interfaces.http.http_api.HttpInterface.__init__","title":"<code>__init__(model_manager, model_registry, root_path=None)</code>","text":"<p>Initializes the HttpInterface with given model manager and model registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> required <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry containing the Roboflow models.</p> required <code>root_path</code> <code>Optional[str]</code> <p>The root path for the FastAPI application.</p> <code>None</code> Description <p>Deploy Roboflow trained models to nearly any compute environment!</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def __init__(\n    self,\n    model_manager: ModelManager,\n    model_registry: RoboflowModelRegistry,\n    root_path: Optional[str] = None,\n):\n\"\"\"\n    Initializes the HttpInterface with given model manager and model registry.\n\n    Args:\n        model_manager (ModelManager): The manager for handling different models.\n        model_registry (RoboflowModelRegistry): The registry containing the Roboflow models.\n        root_path (Optional[str]): The root path for the FastAPI application.\n\n    Description:\n        Deploy Roboflow trained models to nearly any compute environment!\n    \"\"\"\n    description = \"Roboflow inference server\"\n    app = FastAPI(\n        title=\"Roboflow Inference Server\",\n        description=description,\n        version=__version__,\n        terms_of_service=\"https://roboflow.com/terms\",\n        contact={\n            \"name\": \"Roboflow Inc.\",\n            \"url\": \"https://roboflow.com/contact\",\n            \"email\": \"help@roboflow.com\",\n        },\n        license_info={\n            \"name\": \"Apache 2.0\",\n            \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n        },\n        root_path=root_path,\n    )\n    if METLO_KEY:\n        app.add_middleware(\n            ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n        )\n\n    if len(ALLOW_ORIGINS) &gt; 0:\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=ALLOW_ORIGINS,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n    # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n    if PROFILE:\n        app.add_middleware(\n            CProfileMiddleware,\n            enable=True,\n            server_app=app,\n            filename=\"/profile/output.pstats\",\n            strip_dirs=False,\n            sort_by=\"cumulative\",\n        )\n\n    if PINGBACK_ENABLED:\n\n        @app.middleware(\"http\")\n        async def count_errors(request: Request, call_next):\n\"\"\"Middleware to count errors.\n\n            Args:\n                request (Request): The incoming request.\n                call_next (Callable): The next middleware or endpoint to call.\n\n            Returns:\n                Response: The response from the next middleware or endpoint.\n            \"\"\"\n            response = await call_next(request)\n            if response.status_code &gt;= 400:\n                model_manager.model_manager.num_errors += 1\n            return response\n\n    self.app = app\n    self.model_manager = model_manager\n    self.model_registry = model_registry\n\n    def process_inference_request(\n        inference_request: M.InferenceRequest,\n    ) -&gt; M.InferenceResponse:\n\"\"\"Processes an inference request by calling the appropriate model.\n\n        Args:\n            inference_request (M.InferenceRequest): The request containing model ID and other inference details.\n\n        Returns:\n            M.InferenceResponse: The response containing the inference results.\n        \"\"\"\n        if inference_request.model_id not in self.model_manager:\n            model = self.model_registry.get_model(\n                inference_request.model_id, inference_request.api_key\n            )(\n                model_id=inference_request.model_id,\n                api_key=inference_request.api_key,\n            )\n            self.model_manager.add_model(inference_request.model_id, model)\n        return self.model_manager.infer(\n            inference_request.model_id, inference_request\n        )\n\n    def load_core_model(\n        inference_request: M.InferenceRequest,\n        api_key: Optional[str] = None,\n        core_model: str = None,\n    ) -&gt; None:\n\"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n        Args:\n            inference_request (M.InferenceRequest): The request containing version and other details.\n            api_key (Optional[str]): The API key for the request.\n            core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n        Returns:\n            str: The core model ID.\n        \"\"\"\n        if api_key:\n            inference_request.api_key = api_key\n        version_id_field = f\"{core_model}_version_id\"\n        core_model_id = (\n            f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n        )\n        if core_model_id not in self.model_manager:\n            model = self.model_registry.get_model(\n                core_model_id, inference_request.api_key\n            )(\n                model_id=core_model_id,\n                api_key=inference_request.api_key,\n            )\n            self.model_manager.add_model(core_model_id, model)\n        return core_model_id\n\n    load_clip_model = partial(load_core_model, core_model=\"clip\")\n\"\"\"Loads the CLIP model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The CLIP model ID.\n    \"\"\"\n\n    load_sam_model = partial(load_core_model, core_model=\"sam\")\n\"\"\"Loads the SAM model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The SAM model ID.\n    \"\"\"\n\n    @app.get(\n        \"/\",\n        response_model=M.ServerVersionInfo,\n        summary=\"Root\",\n        description=\"Get the server name and version number\",\n    )\n    async def root():\n\"\"\"Endpoint to get the server name and version number.\n\n        Returns:\n            M.ServerVersionInfo: The server version information.\n        \"\"\"\n        return M.ServerVersionInfo(\n            name=\"Roboflow Inference Server\",\n            version=__version__,\n            uuid=model_manager.model_manager.uuid,\n        )\n\n    # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n    if not LAMBDA:\n\n        @app.get(\n            \"/model/registry\",\n            response_model=M.ModelManagerKeys,\n            summary=\"Get model keys\",\n            description=\"Get the ID of each loaded model\",\n        )\n        async def registry():\n\"\"\"Get the ID of each loaded model in the registry.\n\n            Returns:\n                M.ModelManagerKeys: The object containing the IDs of all loaded models.\n            \"\"\"\n\n            return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n        @app.post(\n            \"/model/add\",\n            response_model=M.ModelManagerKeys,\n            summary=\"Load a model\",\n            description=\"Load the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_add(request: M.AddModelRequest):\n\"\"\"Load the model with the given model ID into the model manager.\n\n            Args:\n                request (M.AddModelRequest): The request containing the model ID and optional API key.\n\n            Returns:\n                M.ModelManagerKeys: The object containing the IDs of all loaded models.\n            \"\"\"\n\n            if request.model_id not in self.model_manager:\n                model_class = self.model_registry.get_model(\n                    request.model_id, request.api_key\n                )\n                model = model_class(\n                    model_id=request.model_id, api_key=request.api_key\n                )\n                self.model_manager.add_model(request.model_id, model)\n\n            return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n        @app.post(\n            \"/model/remove\",\n            response_model=M.ModelManagerKeys,\n            summary=\"Remove a model\",\n            description=\"Remove the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_remove(request: M.ClearModelRequest):\n\"\"\"Remove the model with the given model ID from the model manager.\n\n            Args:\n                request (M.ClearModelRequest): The request containing the model ID to be removed.\n\n            Returns:\n                M.ModelManagerKeys: The object containing the IDs of all loaded models.\n            \"\"\"\n\n            self.model_manager.remove(request.model_id)\n            return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n        @app.post(\n            \"/model/clear\",\n            response_model=M.ModelManagerKeys,\n            summary=\"Remove all models\",\n            description=\"Remove all loaded models\",\n        )\n        @with_route_exceptions\n        async def model_clear():\n\"\"\"Remove all loaded models from the model manager.\n\n            Returns:\n                M.ModelManagerKeys: The object containing the IDs of all loaded models (empty set in this case).\n            \"\"\"\n\n            self.model_manager.clear()\n            return M.ModelManagerKeys(model_ids=set(self.model_manager.keys()))\n\n        @app.post(\n            \"/infer/object_detection\",\n            response_model=Union[\n                M.ObjectDetectionInferenceResponse,\n                List[M.ObjectDetectionInferenceResponse],\n            ],\n            summary=\"Object detection infer\",\n            description=\"Run inference with the specified object detection model\",\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def infer_object_detection(\n            inference_request: M.ObjectDetectionInferenceRequest,\n        ):\n\"\"\"Run inference with the specified object detection model.\n\n            Args:\n                inference_request (M.ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n\n            Returns:\n                Union[M.ObjectDetectionInferenceResponse, List[M.ObjectDetectionInferenceResponse]]: The response containing the inference results.\n            \"\"\"\n\n            return process_inference_request(inference_request)\n\n        @app.post(\n            \"/infer/instance_segmentation\",\n            response_model=M.InstanceSegmentationInferenceResponse,\n            summary=\"Instance segmentation infer\",\n            description=\"Run inference with the specified instance segmentation model\",\n        )\n        @with_route_exceptions\n        async def infer_instance_segmentation(\n            inference_request: M.InstanceSegmentationInferenceRequest,\n        ):\n\"\"\"Run inference with the specified instance segmentation model.\n\n            Args:\n                inference_request (M.InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n\n            Returns:\n                M.InstanceSegmentationInferenceResponse: The response containing the inference results.\n            \"\"\"\n\n            return process_inference_request(inference_request)\n\n        @app.post(\n            \"/infer/classification\",\n            response_model=Union[\n                M.ClassificationInferenceResponse,\n                M.MultiLabelClassificationInferenceResponse,\n            ],\n            summary=\"Classification infer\",\n            description=\"Run inference with the specified classification model\",\n        )\n        @with_route_exceptions\n        async def infer_classification(\n            inference_request: M.ClassificationInferenceRequest,\n        ):\n\"\"\"Run inference with the specified classification model.\n\n            Args:\n                inference_request (M.ClassificationInferenceRequest): The request containing the necessary details for classification.\n\n            Returns:\n                Union[M.ClassificationInferenceResponse, M.MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n\n            return process_inference_request(inference_request)\n\n    if CORE_MODELS_ENABLED:\n        if CORE_MODEL_CLIP_ENABLED:\n\n            @app.post(\n                \"/clip/embed_image\",\n                response_model=M.ClipEmbeddingResponse,\n                summary=\"CLIP Image Embeddings\",\n                description=\"Run the Open AI CLIP model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_image(\n                inference_request: M.ClipImageEmbeddingRequest,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                request: Request = Body(),\n            ):\n\"\"\"\n                Embeds image data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (M.ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.ClipEmbeddingResponse: The response containing the embedded image.\n                \"\"\"\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = self.model_manager.infer(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/embed_text\",\n                response_model=M.ClipEmbeddingResponse,\n                summary=\"CLIP Text Embeddings\",\n                description=\"Run the Open AI CLIP model to embed text data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_text(\n                inference_request: M.ClipTextEmbeddingRequest,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                request: Request = Body(),\n            ):\n\"\"\"\n                Embeds text data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (M.ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.ClipEmbeddingResponse: The response containing the embedded text.\n                \"\"\"\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = self.model_manager.infer(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/compare\",\n                response_model=M.ClipCompareResponse,\n                summary=\"CLIP Compare\",\n                description=\"Run the Open AI CLIP model to compute similarity scores.\",\n            )\n            @with_route_exceptions\n            async def clip_compare(\n                inference_request: M.ClipCompareRequest,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                request: Request = Body(),\n            ):\n\"\"\"\n                Computes similarity scores using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (M.ClipCompareRequest): The request containing the data to be compared.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.ClipCompareResponse: The response containing the similarity scores.\n                \"\"\"\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = self.model_manager.infer(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor, n=2)\n                return response\n\n        if CORE_MODEL_SAM_ENABLED:\n\n            @app.post(\n                \"/sam/embed_image\",\n                response_model=M.SamEmbeddingResponse,\n                summary=\"SAM Image Embeddings\",\n                description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def sam_embed_image(\n                inference_request: M.SamEmbeddingRequest,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                request: Request = Body(),\n            ):\n\"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (M.SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                \"\"\"\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = self.model_manager.infer(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response.embeddings,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n            @app.post(\n                \"/sam/segment_image\",\n                response_model=M.SamSegmentationResponse,\n                summary=\"SAM Image Segmentation\",\n                description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            async def sam_segment_image(\n                inference_request: M.SamSegmentationRequest,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                request: Request = Body(),\n            ):\n\"\"\"\n                Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (M.SamSegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = self.model_manager.infer(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n    if LEGACY_ROUTE_ENABLED:\n        # Legacy object detection inference path for backwards compatability\n        @app.post(\n            \"/{dataset_id}/{version_id}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                M.InstanceSegmentationInferenceResponse,\n                M.ObjectDetectionInferenceResponse,\n                M.ClassificationInferenceResponse,\n                M.MultiLabelClassificationInferenceResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def legacy_infer(\n            dataset_id: str = Path(\n                description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n            ),\n            version_id: str = Path(\n                description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n            ),\n            api_key: Optional[str] = Query(\n                None,\n                description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n            ),\n            confidence: float = Query(\n                0.4,\n                description=\"The confidence threshold used to filter out predictions\",\n            ),\n            format: str = Query(\n                \"json\",\n                description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n            ),\n            image: Optional[str] = Query(\n                None,\n                description=\"The publically accessible URL of an image to use for inference.\",\n            ),\n            image_type: Optional[str] = Query(\n                \"base64\",\n                description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n            ),\n            labels: Optional[bool] = Query(\n                False,\n                description=\"If true, labels will be include in any inference visualization.\",\n            ),\n            mask_decode_mode: Optional[str] = Query(\n                \"accurate\",\n                description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n            ),\n            tradeoff_factor: Optional[float] = Query(\n                0.0,\n                description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n            ),\n            max_detections: int = Query(\n                300,\n                description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n            ),\n            overlap: float = Query(\n                0.3,\n                description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n            ),\n            request: Request = Body(),\n            stroke: int = Query(\n                1, description=\"The stroke width used when visualizing predictions\"\n            ),\n            countinference: Optional[bool] = Query(\n                True,\n                description=\"If false, does not track inference against usage.\",\n                include_in_schema=False,\n            ),\n            service_secret: Optional[str] = Query(\n                None,\n                description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                include_in_schema=False,\n            ),\n        ):\n\"\"\"\n            Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n            Args:\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                # Other parameters described in the function signature...\n\n            Returns:\n                Union[M.InstanceSegmentationInferenceResponse, M.ObjectDetectionInferenceResponse, M.ClassificationInferenceResponse, M.MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n            \"\"\"\n            model_id = f\"{dataset_id}/{version_id}\"\n\n            if confidence &gt;= 1:\n                confidence /= 100\n            elif confidence &lt; 0.01:\n                confidence = 0.01\n\n            if overlap &gt;= 1:\n                overlap /= 100\n\n            if image is not None:\n                request_image = M.InferenceRequestImage(type=\"url\", value=image)\n            else:\n                if \"Content-Type\" not in request.headers:\n                    raise ContentTypeMissing(\n                        f\"Request must include a Content-Type header\"\n                    )\n                if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                    form_data = await request.form()\n                    base64_image_str = form_data[\"file\"].file\n                    request_image = M.InferenceRequestImage(\n                        type=\"multipart\", value=base64_image_str\n                    )\n                elif (\n                    \"application/x-www-form-urlencoded\"\n                    in request.headers[\"Content-Type\"]\n                    or \"application/json\" in request.headers[\"Content-Type\"]\n                ):\n                    data = await request.body()\n                    request_image = M.InferenceRequestImage(\n                        type=image_type, value=data\n                    )\n                else:\n                    raise ContentTypeInvalid(\n                        f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                    )\n\n            if LAMBDA:\n                request_model_id = (\n                    request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"model\"][\"endpoint\"]\n                    .replace(\"--\", \"/\")\n                    .replace(\"rf-\", \"\")\n                    .replace(\"nu-\", \"\")\n                )\n                actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                    \"lambda\"\n                ][\"actor\"]\n                if countinference:\n                    trackUsage(request_model_id, actor)\n                else:\n                    if service_secret != ROBOFLOW_SERVICE_SECRET:\n                        raise MissingServiceSecretError(\n                            \"Service secret is required to disable inference usage tracking\"\n                        )\n            else:\n                request_model_id = model_id\n\n            if request_model_id not in self.model_manager:\n                model = self.model_registry.get_model(model_id, api_key)(\n                    model_id=request_model_id, api_key=api_key\n                )\n                self.model_manager.add_model(request_model_id, model)\n\n            task_type = self.model_manager.get_task_type(request_model_id)\n            inference_request_type = M.ObjectDetectionInferenceRequest\n            args = dict()\n            if task_type == \"instance-segmentation\":\n                inference_request_type = M.InstanceSegmentationInferenceRequest\n                args = {\n                    \"mask_decode_mode\": mask_decode_mode,\n                    \"tradeoff_factor\": tradeoff_factor,\n                }\n            elif task_type == \"classification\":\n                inference_request_type = M.ClassificationInferenceRequest\n\n            inference_request = inference_request_type(\n                model_id=request_model_id,\n                image=request_image,\n                confidence=confidence,\n                iou_threshold=overlap,\n                max_detections=max_detections,\n                visualization_labels=labels,\n                visualization_stroke_width=stroke,\n                visualize_predictions=True if format == \"image\" else False,\n                **args,\n            )\n\n            inference_response = self.model_manager.infer(\n                inference_request.model_id, inference_request\n            )\n\n            if format == \"image\":\n                return Response(\n                    content=inference_response.visualization,\n                    media_type=\"image/jpeg\",\n                )\n            else:\n                return inference_response\n\n    if not LAMBDA:\n        # Legacy clear cache endpoint for backwards compatability\n        @app.get(\"/clear_cache\", response_model=str)\n        async def legacy_clear_cache():\n\"\"\"\n            Clears the model cache.\n\n            This endpoint provides a way to clear the cache of loaded models.\n\n            Returns:\n                str: A string indicating that the cache has been cleared.\n            \"\"\"\n            await model_clear()\n            return \"Cache Cleared\"\n\n        # Legacy add model endpoint for backwards compatability\n        @app.get(\"/start/{dataset_id}/{version_id}\")\n        async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n\"\"\"\n            Starts a model inference session.\n\n            This endpoint initializes and starts an inference session for the specified model version.\n\n            Args:\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n            Returns:\n                JSONResponse: A response object containing the status and a success message.\n            \"\"\"\n            model_id = f\"{dataset_id}/{version_id}\"\n            model = self.model_registry.get_model(model_id, api_key)(\n                model_id=model_id, api_key=api_key\n            )\n            self.model_manager.add_model(model_id, model)\n\n            return JSONResponse(\n                {\n                    \"status\": 200,\n                    \"message\": \"inference session started from local memory.\",\n                }\n            )\n</code></pre>"},{"location":"library/interfaces/udp/","title":"UDP Stream Interface","text":"<p>         Bases: <code>BaseInterface</code></p> <p>Roboflow defined UDP interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>ip_broadcast_addr</code> <code>str</code> <p>The IP address to broadcast to.</p> <code>ip_broadcast_port</code> <code>int</code> <p>The port to broadcast on.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>json_response</code> <code>bool</code> <p>Flag to toggle JSON response format.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> Methods <p>init_infer: Initialize the inference with a test frame. preprocess_thread: Preprocess incoming frames for inference. inference_request_thread: Manage the inference requests. run_thread: Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>class UdpStream(BaseInterface):\n\"\"\"Roboflow defined UDP interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        ip_broadcast_addr (str): The IP address to broadcast to.\n        ip_broadcast_port (int): The port to broadcast on.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        json_response (bool): Flag to toggle JSON response format.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model_id (str): The ID of the model to be used.\n        stream_id (str): The ID of the stream to be used.\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        model_registry: RoboflowModelRegistry,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n        ip_broadcast_port: int = IP_BROADCAST_PORT,\n        iou_threshold: float = IOU_THRESHOLD,\n        json_response: bool = JSON_RESPONSE,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model_id: str = MODEL_ID,\n        stream_id: str = STREAM_ID,\n    ):\n\"\"\"Initialize the UDP stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        print(\"Initializing server\")\n\n        self.model_manager = model_manager\n        self.model_registry = model_registry\n        self.frame_count = 0\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model_id\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n        if not self.api_key:\n            raise ValueError(\"API_KEY is not defined\")\n\n        model = self.model_registry.get_model(self.model_id, self.api_key)(\n            model_id=self.model_id,\n            api_key=self.api_key,\n        )\n        self.model_manager.add_model(self.model_id, model)\n\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.ip_broadcast_addr = ip_broadcast_addr\n        self.ip_broadcast_port = ip_broadcast_port\n        self.json_response = json_response\n\n        self.inference_request_type = M.ObjectDetectionInferenceRequest\n\n        self.UDPServerSocket = socket.socket(\n            family=socket.AF_INET, type=socket.SOCK_DGRAM\n        )\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 1)\n\n        self.webcam_stream = WebcamStream(stream_id=self.stream_id)\n        print(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame = None\n        self.frame_cv = None\n        self.frame_id = None\n        print(\"Server initialized with settings:\")\n        print(f\"Stream ID: {self.stream_id}\")\n        print(f\"Model ID: {self.model_id}\")\n        print(f\"Confidence: {self.confidence}\")\n        print(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        print(f\"IOU Threshold: {self.iou_threshold}\")\n        print(f\"Max Candidates: {self.max_candidates}\")\n        print(f\"Max Detections: {self.max_detections}\")\n\n    def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        request_image = M.InferenceRequestImage(type=\"pil\", value=frame)\n        inference_request_obj = self.inference_request_type(\n            model_id=self.model_id,\n            image=request_image,\n            api_key=self.api_key,\n        )\n        preproc_result = self.model_manager.preprocess(\n            inference_request_obj.model_id, inference_request_obj\n        )\n        img_in, img_dims = preproc_result\n        predictions = self.model_manager.predict(\n            inference_request_obj.model_id,\n            img_in,\n        )\n        predictions = self.model_manager.postprocess(\n            inference_request_obj.model_id,\n            predictions,\n            img_dims,\n            class_agnostic_nms=self.class_agnostic_nms,\n            confidence=self.confidence,\n            iou_threshold=self.iou_threshold,\n            max_candidates=self.max_candidates,\n            max_detections=self.max_detections,\n        )\n\n    def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame, self.frame_cv, frame_id = webcam_stream.read()\n                    if frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        request_image = M.InferenceRequestImage(\n                            type=\"pil\", value=self.frame\n                        )\n                        self.inference_request_obj = self.inference_request_type(\n                            model_id=self.model_id,\n                            image=request_image,\n                            api_key=self.api_key,\n                        )\n                        self.preproc_result = self.model_manager.preprocess(\n                            self.inference_request_obj.model_id,\n                            self.inference_request_obj,\n                        )\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            print(e)\n\n    def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        as a UDP broadcast.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            if self.stop:\n                break\n            if self.queue_control:\n                self.queue_control = False\n                frame_id = self.frame_id\n                predictions = self.model_manager.predict(\n                    self.inference_request_obj.model_id,\n                    self.img_in,\n                )\n                predictions = self.model_manager.postprocess(\n                    self.inference_request_obj.model_id,\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )\n                if self.json_response:\n                    predictions = self.model_manager.make_response(\n                        self.inference_request_obj.model_id,\n                        predictions,\n                        self.img_dims,\n                    )[0]\n                    predictions.frame_id = frame_id\n                    predictions = predictions.json(exclude_none=True)\n                else:\n                    predictions = json.dumps(predictions)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                bytesToSend = predictions.encode(\"utf-8\")\n                self.UDPServerSocket.sendto(\n                    bytesToSend,\n                    (\n                        self.ip_broadcast_addr,\n                        self.ip_broadcast_port,\n                    ),\n                )\n\n    def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n\n        preprocess_thread.start()\n        inference_request_thread.start()\n\n        while True:\n            try:\n                time.sleep(10)\n            except KeyboardInterrupt:\n                print(\"Stopping server...\")\n                self.stop = True\n                time.sleep(3)\n                sys.exit(0)\n</code></pre>"},{"location":"library/interfaces/udp/#inference.core.interfaces.udp.udp_stream.UdpStream.__init__","title":"<code>__init__(model_manager, model_registry, api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, ip_broadcast_addr=IP_BROADCAST_ADDR, ip_broadcast_port=IP_BROADCAST_PORT, iou_threshold=IOU_THRESHOLD, json_response=JSON_RESPONSE, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model_id=MODEL_ID, stream_id=STREAM_ID)</code>","text":"<p>Initialize the UDP stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def __init__(\n    self,\n    model_manager: ModelManager,\n    model_registry: RoboflowModelRegistry,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n    ip_broadcast_port: int = IP_BROADCAST_PORT,\n    iou_threshold: float = IOU_THRESHOLD,\n    json_response: bool = JSON_RESPONSE,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model_id: str = MODEL_ID,\n    stream_id: str = STREAM_ID,\n):\n\"\"\"Initialize the UDP stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    print(\"Initializing server\")\n\n    self.model_manager = model_manager\n    self.model_registry = model_registry\n    self.frame_count = 0\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model_id\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n    if not self.api_key:\n        raise ValueError(\"API_KEY is not defined\")\n\n    model = self.model_registry.get_model(self.model_id, self.api_key)(\n        model_id=self.model_id,\n        api_key=self.api_key,\n    )\n    self.model_manager.add_model(self.model_id, model)\n\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.ip_broadcast_addr = ip_broadcast_addr\n    self.ip_broadcast_port = ip_broadcast_port\n    self.json_response = json_response\n\n    self.inference_request_type = M.ObjectDetectionInferenceRequest\n\n    self.UDPServerSocket = socket.socket(\n        family=socket.AF_INET, type=socket.SOCK_DGRAM\n    )\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 1)\n\n    self.webcam_stream = WebcamStream(stream_id=self.stream_id)\n    print(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame = None\n    self.frame_cv = None\n    self.frame_id = None\n    print(\"Server initialized with settings:\")\n    print(f\"Stream ID: {self.stream_id}\")\n    print(f\"Model ID: {self.model_id}\")\n    print(f\"Confidence: {self.confidence}\")\n    print(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    print(f\"IOU Threshold: {self.iou_threshold}\")\n    print(f\"Max Candidates: {self.max_candidates}\")\n    print(f\"Max Detections: {self.max_detections}\")\n</code></pre>"},{"location":"library/interfaces/udp/#inference.core.interfaces.udp.udp_stream.UdpStream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results as a UDP broadcast.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    as a UDP broadcast.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        if self.stop:\n            break\n        if self.queue_control:\n            self.queue_control = False\n            frame_id = self.frame_id\n            predictions = self.model_manager.predict(\n                self.inference_request_obj.model_id,\n                self.img_in,\n            )\n            predictions = self.model_manager.postprocess(\n                self.inference_request_obj.model_id,\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )\n            if self.json_response:\n                predictions = self.model_manager.make_response(\n                    self.inference_request_obj.model_id,\n                    predictions,\n                    self.img_dims,\n                )[0]\n                predictions.frame_id = frame_id\n                predictions = predictions.json(exclude_none=True)\n            else:\n                predictions = json.dumps(predictions)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            bytesToSend = predictions.encode(\"utf-8\")\n            self.UDPServerSocket.sendto(\n                bytesToSend,\n                (\n                    self.ip_broadcast_addr,\n                    self.ip_broadcast_port,\n                ),\n            )\n</code></pre>"},{"location":"library/interfaces/udp/#inference.core.interfaces.udp.udp_stream.UdpStream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    request_image = M.InferenceRequestImage(type=\"pil\", value=frame)\n    inference_request_obj = self.inference_request_type(\n        model_id=self.model_id,\n        image=request_image,\n        api_key=self.api_key,\n    )\n    preproc_result = self.model_manager.preprocess(\n        inference_request_obj.model_id, inference_request_obj\n    )\n    img_in, img_dims = preproc_result\n    predictions = self.model_manager.predict(\n        inference_request_obj.model_id,\n        img_in,\n    )\n    predictions = self.model_manager.postprocess(\n        inference_request_obj.model_id,\n        predictions,\n        img_dims,\n        class_agnostic_nms=self.class_agnostic_nms,\n        confidence=self.confidence,\n        iou_threshold=self.iou_threshold,\n        max_candidates=self.max_candidates,\n        max_detections=self.max_detections,\n    )\n</code></pre>"},{"location":"library/interfaces/udp/#inference.core.interfaces.udp.udp_stream.UdpStream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame, self.frame_cv, frame_id = webcam_stream.read()\n                if frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    request_image = M.InferenceRequestImage(\n                        type=\"pil\", value=self.frame\n                    )\n                    self.inference_request_obj = self.inference_request_type(\n                        model_id=self.model_id,\n                        image=request_image,\n                        api_key=self.api_key,\n                    )\n                    self.preproc_result = self.model_manager.preprocess(\n                        self.inference_request_obj.model_id,\n                        self.inference_request_obj,\n                    )\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"library/interfaces/udp/#inference.core.interfaces.udp.udp_stream.UdpStream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    inference_request_thread = threading.Thread(\n        target=self.inference_request_thread\n    )\n\n    preprocess_thread.start()\n    inference_request_thread.start()\n\n    while True:\n        try:\n            time.sleep(10)\n        except KeyboardInterrupt:\n            print(\"Stopping server...\")\n            self.stop = True\n            time.sleep(3)\n            sys.exit(0)\n</code></pre>"},{"location":"library/model_managers/base/","title":"Base Model Manager","text":"<p>Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>@dataclass\nclass ModelManager:\n\"\"\"Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.\"\"\"\n\n    _models: Dict[str, Model] = field(default_factory=dict)\n\n    def init_pingback(self):\n\"\"\"Initializes pingback mechanism.\"\"\"\n        self.num_errors = 0  # in the device\n        self.uuid = ROBOFLOW_SERVER_UUID\n        self.pingback = PingbackInfo(self)\n        self.pingback.start()\n\n    def add_model(self, model_id: str, model: Model) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        if model_id in self._models:\n            return\n        self._models[model_id] = model\n\n    def check_for_model(self, model_id: str) -&gt; None:\n\"\"\"Checks whether the model with the given ID is in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Raises:\n            InferenceModelNotFound: If the model is not found in the manager.\n        \"\"\"\n        if model_id not in self:\n            raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n\n    def infer(self, model_id: str, request: InferenceRequest) -&gt; InferenceResponse:\n\"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        self.check_for_model(model_id)\n        self._models[model_id].metrics[\"num_inferences\"] += 1\n        tic = time.perf_counter()\n        rtn_val = self._models[model_id].infer(request)\n        toc = time.perf_counter()\n        self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n        return rtn_val\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Creates a response object from the model's predictions.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (List[List[float]]): The model's predictions.\n\n        Returns:\n            InferenceResponse: The created response object.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].make_response(predictions, *args, **kwargs)\n\n    def postprocess(\n        self, model_id: str, predictions: np.ndarray, *args, **kwargs\n    ) -&gt; List[List[float]]:\n\"\"\"Processes the model's predictions after inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (np.ndarray): The model's predictions.\n\n        Returns:\n            List[List[float]]: The post-processed predictions.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].postprocess(predictions, *args, **kwargs)\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; np.ndarray:\n\"\"\"Runs prediction on the specified model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            np.ndarray: The predictions from the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        self._models[model_id].metrics[\"num_inferences\"] += 1\n        tic = time.perf_counter()\n        res = self._models[model_id].predict(*args, **kwargs)\n        toc = time.perf_counter()\n        self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n        return res\n\n    def preprocess(\n        self, model_id: str, request: InferenceRequest\n    ) -&gt; Tuple[np.ndarray, List[Tuple[int, int]]]:\n\"\"\"Preprocesses the request before inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].preprocess(request)\n\n    def get_class_names(self, model_id):\n\"\"\"Retrieves the class names for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            List[str]: The class names of the model.\n        \"\"\"\n        return self._models[model_id].class_names\n\n    def get_task_type(self, model_id: str) -&gt; str:\n\"\"\"Retrieves the task type for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type of the model.\n        \"\"\"\n        return self._models[model_id].task_type\n\n    def remove(self, model_id: str) -&gt; None:\n\"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n        \"\"\"\n        self._models[model_id].clear_cache()\n        del self._models[model_id]\n\n    def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n        for model_id in list(self.keys()):\n            self.remove(model_id)\n\n    def __contains__(self, model_id: str) -&gt; bool:\n\"\"\"Checks if the model is contained in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: Whether the model is in the manager.\n        \"\"\"\n        return model_id in self._models\n\n    def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieve a model from the manager by key.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model corresponding to the key.\n        \"\"\"\n        return self._models[key]\n\n    def __len__(self) -&gt; int:\n\"\"\"Retrieve the number of models in the manager.\n\n        Returns:\n            int: The number of models in the manager.\n        \"\"\"\n        return len(self._models)\n\n    def keys(self):\n\"\"\"Retrieve the keys (model identifiers) from the manager.\n\n        Returns:\n            List[str]: The keys of the models in the manager.\n        \"\"\"\n        return self._models.keys()\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if the model is contained in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the model is in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __contains__(self, model_id: str) -&gt; bool:\n\"\"\"Checks if the model is contained in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: Whether the model is in the manager.\n    \"\"\"\n    return model_id in self._models\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve a model from the manager by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model corresponding to the key.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieve a model from the manager by key.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model corresponding to the key.\n    \"\"\"\n    return self._models[key]\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.__len__","title":"<code>__len__()</code>","text":"<p>Retrieve the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Retrieve the number of models in the manager.\n\n    Returns:\n        int: The number of models in the manager.\n    \"\"\"\n    return len(self._models)\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.add_model","title":"<code>add_model(model_id, model)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def add_model(self, model_id: str, model: Model) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    if model_id in self._models:\n        return\n    self._models[model_id] = model\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.check_for_model","title":"<code>check_for_model(model_id)</code>","text":"<p>Checks whether the model with the given ID is in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Raises:</p> Type Description <code>InferenceModelNotFound</code> <p>If the model is not found in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def check_for_model(self, model_id: str) -&gt; None:\n\"\"\"Checks whether the model with the given ID is in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Raises:\n        InferenceModelNotFound: If the model is not found in the manager.\n    \"\"\"\n    if model_id not in self:\n        raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n    for model_id in list(self.keys()):\n        self.remove(model_id)\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Retrieves the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List[str]: The class names of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_class_names(self, model_id):\n\"\"\"Retrieves the class names for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        List[str]: The class names of the model.\n    \"\"\"\n    return self._models[model_id].class_names\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.get_task_type","title":"<code>get_task_type(model_id)</code>","text":"<p>Retrieves the task type for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_task_type(self, model_id: str) -&gt; str:\n\"\"\"Retrieves the task type for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type of the model.\n    \"\"\"\n    return self._models[model_id].task_type\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.infer","title":"<code>infer(model_id, request)</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def infer(self, model_id: str, request: InferenceRequest) -&gt; InferenceResponse:\n\"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    self.check_for_model(model_id)\n    self._models[model_id].metrics[\"num_inferences\"] += 1\n    tic = time.perf_counter()\n    rtn_val = self._models[model_id].infer(request)\n    toc = time.perf_counter()\n    self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n    return rtn_val\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.init_pingback","title":"<code>init_pingback()</code>","text":"<p>Initializes pingback mechanism.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def init_pingback(self):\n\"\"\"Initializes pingback mechanism.\"\"\"\n    self.num_errors = 0  # in the device\n    self.uuid = ROBOFLOW_SERVER_UUID\n    self.pingback = PingbackInfo(self)\n    self.pingback.start()\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.keys","title":"<code>keys()</code>","text":"<p>Retrieve the keys (model identifiers) from the manager.</p> <p>Returns:</p> Type Description <p>List[str]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def keys(self):\n\"\"\"Retrieve the keys (model identifiers) from the manager.\n\n    Returns:\n        List[str]: The keys of the models in the manager.\n    \"\"\"\n    return self._models.keys()\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.make_response","title":"<code>make_response(model_id, predictions, *args, **kwargs)</code>","text":"<p>Creates a response object from the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>List[List[float]]</code> <p>The model's predictions.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The created response object.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def make_response(\n    self, model_id: str, predictions: List[List[float]], *args, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Creates a response object from the model's predictions.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (List[List[float]]): The model's predictions.\n\n    Returns:\n        InferenceResponse: The created response object.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].make_response(predictions, *args, **kwargs)\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.postprocess","title":"<code>postprocess(model_id, predictions, *args, **kwargs)</code>","text":"<p>Processes the model's predictions after inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>np.ndarray</code> <p>The model's predictions.</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: The post-processed predictions.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def postprocess(\n    self, model_id: str, predictions: np.ndarray, *args, **kwargs\n) -&gt; List[List[float]]:\n\"\"\"Processes the model's predictions after inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (np.ndarray): The model's predictions.\n\n    Returns:\n        List[List[float]]: The post-processed predictions.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].postprocess(predictions, *args, **kwargs)\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.predict","title":"<code>predict(model_id, *args, **kwargs)</code>","text":"<p>Runs prediction on the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The predictions from the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def predict(self, model_id: str, *args, **kwargs) -&gt; np.ndarray:\n\"\"\"Runs prediction on the specified model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        np.ndarray: The predictions from the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    self._models[model_id].metrics[\"num_inferences\"] += 1\n    tic = time.perf_counter()\n    res = self._models[model_id].predict(*args, **kwargs)\n    toc = time.perf_counter()\n    self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n    return res\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Preprocesses the request before inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, List[Tuple[int, int]]]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def preprocess(\n    self, model_id: str, request: InferenceRequest\n) -&gt; Tuple[np.ndarray, List[Tuple[int, int]]]:\n\"\"\"Preprocesses the request before inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].preprocess(request)\n</code></pre>"},{"location":"library/model_managers/base/#inference.core.managers.base.ModelManager.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def remove(self, model_id: str) -&gt; None:\n\"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n    \"\"\"\n    self._models[model_id].clear_cache()\n    del self._models[model_id]\n</code></pre>"},{"location":"library/model_managers/pingback/","title":"Inference Server Pingback Manager","text":"<p>Class responsible for managing pingback information for Roboflow.</p> <p>This class initializes a scheduler to periodically post data to Roboflow, containing information about the models, container, and device.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>BackgroundScheduler</code> <p>A scheduler for running jobs in the background.</p> <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> <code>process_startup_time</code> <code>str</code> <p>Unix timestamp indicating when the process started.</p> <code>pingback_url</code> <code>str</code> <p>URL to send the pingback data to.</p> <code>system_info</code> <code>dict</code> <p>Information about the system.</p> <code>window_start_timestamp</code> <code>str</code> <p>Unix timestamp indicating the start of the current window.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>class PingbackInfo:\n\"\"\"Class responsible for managing pingback information for Roboflow.\n\n    This class initializes a scheduler to periodically post data to Roboflow, containing information about the models,\n    container, and device.\n\n    Attributes:\n        scheduler (BackgroundScheduler): A scheduler for running jobs in the background.\n        model_manager (ModelManager): Reference to the model manager object.\n        process_startup_time (str): Unix timestamp indicating when the process started.\n        pingback_url (str): URL to send the pingback data to.\n        system_info (dict): Information about the system.\n        window_start_timestamp (str): Unix timestamp indicating the start of the current window.\n    \"\"\"\n\n    def __init__(self, manager):\n\"\"\"Initializes PingbackInfo with the given manager.\n\n        Args:\n            manager (ModelManager): Reference to the model manager object.\n        \"\"\"\n        try:\n            self.scheduler = BackgroundScheduler()\n            self.model_manager = manager\n            self.process_startup_time = str(int(time.time()))\n            logger.info(\n                \"UUID: \" + self.model_manager.uuid\n            )  # To correlate with UI container view\n            self.pingback_url = PINGBACK_URL  # Test URL\n\n            self.system_info = getSystemInfo()\n            self.window_start_timestamp = str(int(time.time()))\n        except Exception as e:\n            logger.error(\n                \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n                + str(e)\n            )\n\n    def start(self):\n\"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n        If PINGBACK_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n        \"\"\"\n        if PINGBACK_ENABLED == False:\n            logger.warn(\n                \"Pingback to Roboflow is disabled; not sending back stats to Roboflow.\"\n            )\n            return\n        try:\n            self.scheduler.add_job(\n                self.post_data,\n                \"interval\",\n                seconds=PINGBACK_INTERVAL_SECONDS,\n                args=[self.model_manager],\n            )\n            self.scheduler.start()\n        except Exception as e:\n            print(e)\n\n    def stop(self):\n\"\"\"Stops the scheduler.\"\"\"\n        self.scheduler.shutdown()\n\n    def post_data(self, model_manager):\n\"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n        Args:\n            model_manager (ModelManager): Reference to the model manager object.\n\n        The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n        \"\"\"\n        all_data = {}\n        try:\n            all_data = {\n                \"api_key\": API_KEY or \"no_model_used\",\n                \"container\": {\n                    \"startup_time\": self.process_startup_time,\n                    \"uuid\": self.model_manager.uuid,\n                },\n                \"models\": [],\n                \"window_start_timestamp\": self.window_start_timestamp,\n                \"device\": {\n                    \"id\": DEVICE_ID,\n                    \"name\": DEVICE_ID,\n                    \"type\": \"inference_server\",\n                    \"tags\": [],\n                    \"system_info\": self.system_info,\n                },\n                \"num_errors\": self.model_manager.num_errors,\n            }\n            for key in model_manager._models:\n                post_data = {}\n                model = model_manager._models[key]\n                all_data[\"api_key\"] = model.api_key\n                post_data[\"model\"] = {\n                    \"api_key\": model.api_key,\n                    \"dataset_id\": model.dataset_id,\n                    \"version\": model.version_id,\n                }\n                post_data[\"data\"] = {}\n                post_data[\"data\"][\"metrics\"] = {\n                    \"num_inferences\": model.metrics[\"num_inferences\"],\n                    \"avg_inference_time\": model.metrics[\"avg_inference_time\"]\n                    / model.metrics[\"num_inferences\"]\n                    if model.metrics[\"num_inferences\"] &gt; 0\n                    else 0,\n                    \"num_errors\": self.model_manager.num_errors,  # This is not really per model, its per container; kept this for v1\n                }\n                all_data[\"models\"].append(post_data)\n                # Reset metrics\n                model.metrics[\"num_inferences\"] = 0\n                model.metrics[\"avg_inference_time\"] = 0\n                self.model_manager.num_errors = 0\n\n            timestamp = str(int(time.time()))\n            all_data[\"timestamp\"] = timestamp\n            self.window_start_timestamp = timestamp\n            requests.post(PINGBACK_URL, json=all_data)\n            logger.info(\n                \"Sent pingback to Roboflow {} at {}.\".format(\n                    PINGBACK_URL, str(all_data)\n                )\n            )\n\n        except Exception as e:\n            logger.error(\n                \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n                + str(e)\n            )\n</code></pre>"},{"location":"library/model_managers/pingback/#inference.core.managers.pingback.PingbackInfo.__init__","title":"<code>__init__(manager)</code>","text":"<p>Initializes PingbackInfo with the given manager.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def __init__(self, manager):\n\"\"\"Initializes PingbackInfo with the given manager.\n\n    Args:\n        manager (ModelManager): Reference to the model manager object.\n    \"\"\"\n    try:\n        self.scheduler = BackgroundScheduler()\n        self.model_manager = manager\n        self.process_startup_time = str(int(time.time()))\n        logger.info(\n            \"UUID: \" + self.model_manager.uuid\n        )  # To correlate with UI container view\n        self.pingback_url = PINGBACK_URL  # Test URL\n\n        self.system_info = getSystemInfo()\n        self.window_start_timestamp = str(int(time.time()))\n    except Exception as e:\n        logger.error(\n            \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n            + str(e)\n        )\n</code></pre>"},{"location":"library/model_managers/pingback/#inference.core.managers.pingback.PingbackInfo.post_data","title":"<code>post_data(model_manager)</code>","text":"<p>Posts data to Roboflow about the models, container, device, and other relevant metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required <p>The data is collected and reset for the next window, and a POST request is made to the pingback URL.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def post_data(self, model_manager):\n\"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n    Args:\n        model_manager (ModelManager): Reference to the model manager object.\n\n    The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n    \"\"\"\n    all_data = {}\n    try:\n        all_data = {\n            \"api_key\": API_KEY or \"no_model_used\",\n            \"container\": {\n                \"startup_time\": self.process_startup_time,\n                \"uuid\": self.model_manager.uuid,\n            },\n            \"models\": [],\n            \"window_start_timestamp\": self.window_start_timestamp,\n            \"device\": {\n                \"id\": DEVICE_ID,\n                \"name\": DEVICE_ID,\n                \"type\": \"inference_server\",\n                \"tags\": [],\n                \"system_info\": self.system_info,\n            },\n            \"num_errors\": self.model_manager.num_errors,\n        }\n        for key in model_manager._models:\n            post_data = {}\n            model = model_manager._models[key]\n            all_data[\"api_key\"] = model.api_key\n            post_data[\"model\"] = {\n                \"api_key\": model.api_key,\n                \"dataset_id\": model.dataset_id,\n                \"version\": model.version_id,\n            }\n            post_data[\"data\"] = {}\n            post_data[\"data\"][\"metrics\"] = {\n                \"num_inferences\": model.metrics[\"num_inferences\"],\n                \"avg_inference_time\": model.metrics[\"avg_inference_time\"]\n                / model.metrics[\"num_inferences\"]\n                if model.metrics[\"num_inferences\"] &gt; 0\n                else 0,\n                \"num_errors\": self.model_manager.num_errors,  # This is not really per model, its per container; kept this for v1\n            }\n            all_data[\"models\"].append(post_data)\n            # Reset metrics\n            model.metrics[\"num_inferences\"] = 0\n            model.metrics[\"avg_inference_time\"] = 0\n            self.model_manager.num_errors = 0\n\n        timestamp = str(int(time.time()))\n        all_data[\"timestamp\"] = timestamp\n        self.window_start_timestamp = timestamp\n        requests.post(PINGBACK_URL, json=all_data)\n        logger.info(\n            \"Sent pingback to Roboflow {} at {}.\".format(\n                PINGBACK_URL, str(all_data)\n            )\n        )\n\n    except Exception as e:\n        logger.error(\n            \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n            + str(e)\n        )\n</code></pre>"},{"location":"library/model_managers/pingback/#inference.core.managers.pingback.PingbackInfo.start","title":"<code>start()</code>","text":"<p>Starts the scheduler to periodically post data to Roboflow.</p> <p>If PINGBACK_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def start(self):\n\"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n    If PINGBACK_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n    \"\"\"\n    if PINGBACK_ENABLED == False:\n        logger.warn(\n            \"Pingback to Roboflow is disabled; not sending back stats to Roboflow.\"\n        )\n        return\n    try:\n        self.scheduler.add_job(\n            self.post_data,\n            \"interval\",\n            seconds=PINGBACK_INTERVAL_SECONDS,\n            args=[self.model_manager],\n        )\n        self.scheduler.start()\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"library/model_managers/pingback/#inference.core.managers.pingback.PingbackInfo.stop","title":"<code>stop()</code>","text":"<p>Stops the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def stop(self):\n\"\"\"Stops the scheduler.\"\"\"\n    self.scheduler.shutdown()\n</code></pre>"},{"location":"library/model_registries/base/","title":"Base Model Registry","text":"<p>An object which is able to return model classes based on given model IDs and model types.</p> <p>Attributes:</p> Name Type Description <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>class ModelRegistry:\n\"\"\"An object which is able to return model classes based on given model IDs and model types.\n\n    Attributes:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n\n    def __init__(self, registry_dict) -&gt; None:\n\"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n        Args:\n            registry_dict (dict): A dictionary mapping model types to model classes.\n        \"\"\"\n        self.registry_dict = registry_dict\n\n    def get_model(self, model_type: str, model_id: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model type.\n\n        Args:\n            model_type (str): The type of the model to be retrieved.\n            model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n        Returns:\n            Model: The model class corresponding to the given model type.\n\n        Raises:\n            KeyError: If the model_type is not found in the registry_dict.\n        \"\"\"\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"library/model_registries/base/#inference.core.registries.base.ModelRegistry.__init__","title":"<code>__init__(registry_dict)</code>","text":"<p>Initializes the ModelRegistry with the given dictionary of registered models.</p> <p>Parameters:</p> Name Type Description Default <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> required Source code in <code>inference/core/registries/base.py</code> <pre><code>def __init__(self, registry_dict) -&gt; None:\n\"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n    Args:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n    self.registry_dict = registry_dict\n</code></pre>"},{"location":"library/model_registries/base/#inference.core.registries.base.ModelRegistry.get_model","title":"<code>get_model(model_type, model_id)</code>","text":"<p>Returns the model class based on the given model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to be retrieved.</p> required <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved (unused in the current implementation).</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model type.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the model_type is not found in the registry_dict.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>def get_model(self, model_type: str, model_id: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model type.\n\n    Args:\n        model_type (str): The type of the model to be retrieved.\n        model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n    Returns:\n        Model: The model class corresponding to the given model type.\n\n    Raises:\n        KeyError: If the model_type is not found in the registry_dict.\n    \"\"\"\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"library/model_registries/roboflow/","title":"Roboflow Model Registry","text":"<p>         Bases: <code>ModelRegistry</code></p> <p>A Roboflow-specific model registry which gets the model type using the model id, then returns a model class based on the model type.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>class RoboflowModelRegistry(ModelRegistry):\n\"\"\"A Roboflow-specific model registry which gets the model type using the model id,\n    then returns a model class based on the model type.\n    \"\"\"\n\n    def get_model(self, model_id: str, api_key: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model id and API key.\n\n        Args:\n            model_id (str): The ID of the model to be retrieved.\n            api_key (str): The API key used to authenticate.\n\n        Returns:\n            Model: The model class corresponding to the given model ID and type.\n\n        Raises:\n            DatasetLoadError: If the model type is not supported or found.\n        \"\"\"\n        model_type = get_model_type(model_id, api_key)\n        if model_type not in self.registry_dict:\n            raise DatasetLoadError(f\"Model type not supported: {model_type}\")\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"library/model_registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry.get_model","title":"<code>get_model(model_id, api_key)</code>","text":"<p>Returns the model class based on the given model id and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model ID and type.</p> <p>Raises:</p> Type Description <code>DatasetLoadError</code> <p>If the model type is not supported or found.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model(self, model_id: str, api_key: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model id and API key.\n\n    Args:\n        model_id (str): The ID of the model to be retrieved.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        Model: The model class corresponding to the given model ID and type.\n\n    Raises:\n        DatasetLoadError: If the model type is not supported or found.\n    \"\"\"\n    model_type = get_model_type(model_id, api_key)\n    if model_type not in self.registry_dict:\n        raise DatasetLoadError(f\"Model type not supported: {model_type}\")\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"library/models/clip/","title":"CLIP","text":"<p>         Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX Clip model.</p> <p>This class is responsible for handling the ONNX Clip model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>visual_onnx_session</code> <code>onnxruntime.InferenceSession</code> <p>ONNX Runtime session for visual inference.</p> <code>textual_onnx_session</code> <code>onnxruntime.InferenceSession</code> <p>ONNX Runtime session for textual inference.</p> <code>resolution</code> <code>int</code> <p>The resolution of the input image.</p> <code>clip_preprocess</code> <code>function</code> <p>Function to preprocess the image.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>class ClipOnnxRoboflowCoreModel(OnnxRoboflowCoreModel):\n\"\"\"Roboflow ONNX Clip model.\n\n    This class is responsible for handling the ONNX Clip model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        visual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for visual inference.\n        textual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for textual inference.\n        resolution (int): The resolution of the input image.\n        clip_preprocess (function): Function to preprocess the image.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initializes the ClipOnnxRoboflowCoreModel with the given arguments and keyword arguments.\"\"\"\n\n        t1 = perf_counter()\n        super().__init__(*args, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n        self.visual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"visual.onnx\"),\n            providers=[\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                    },\n                ),\n                \"CUDAExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n\n        self.textual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"textual.onnx\"),\n            providers=[\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                    },\n                ),\n                \"CUDAExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n        self.clip_preprocess = _transform(self.resolution)\n        self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n\n    def compare(self, request: ClipCompareRequest) -&gt; ClipCompareResponse:\n\"\"\"Compares the subject with the prompt using the Clip model.\n\n        Args:\n            request (ClipCompareRequest): The request object containing the subject and prompt.\n\n        Returns:\n            ClipCompareResponse: The response object containing the similarity score.\n        \"\"\"\n        t1 = perf_counter()\n        if request.subject_type == \"image\":\n            subject_embed_request = ClipImageEmbeddingRequest(image=request.subject)\n            subject_embeddings = self.embed_image(subject_embed_request).embeddings[0]\n        elif request.subject_type == \"text\":\n            subject_embed_request = ClipTextEmbeddingRequest(text=request.subject)\n            subject_embeddings = self.embed_text(subject_embed_request).embeddings[0]\n        else:\n            raise ValueError(\n                \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n            )\n\n        if isinstance(request.prompt, dict):\n            prompt_keys = request.prompt.keys()\n            prompt = [request.prompt[k] for k in prompt_keys]\n            prompt_obj = \"dict\"\n        else:\n            prompt = request.prompt\n            if not isinstance(prompt, list):\n                prompt = [prompt]\n            prompt_obj = \"list\"\n\n        if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        if request.prompt_type == \"image\":\n            prompt_embed_request = ClipImageEmbeddingRequest(image=prompt)\n            prompt_embeddings = self.embed_image(prompt_embed_request).embeddings\n        elif request.prompt_type == \"text\":\n            prompt_embed_request = ClipTextEmbeddingRequest(text=prompt)\n            prompt_embeddings = self.embed_text(prompt_embed_request).embeddings\n        else:\n            raise ValueError(\n                \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n            )\n\n        similarities = [\n            cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n        ]\n\n        if prompt_obj == \"dict\":\n            similarities = dict(zip(prompt_keys, similarities))\n\n        response = ClipCompareResponse(\n            similarity=similarities, time=perf_counter() - t1\n        )\n\n        return response\n\n    def embed_image(self, request: ClipImageEmbeddingRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Embeds an image using the Clip model.\n\n        Args:\n            request (ClipImageEmbeddingRequest): The request object containing the image.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(request.image, list):\n            if len(request.image) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n            imgs = [self.preproc_image(i) for i in request.image]\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in = self.preproc_image(request.image)\n\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n        response = ClipEmbeddingResponse(\n            embeddings=embeddings.tolist(), time=perf_counter() - t1\n        )\n\n        return response\n\n    def embed_text(self, request: ClipTextEmbeddingRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Embeds a text using the Clip model.\n\n        Args:\n            request (ClipTextEmbeddingRequest): The request object containing the text.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(request.text, list):\n            if len(request.text) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of text strings that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n\n            texts = request.text\n        else:\n            texts = [request.text]\n\n        texts = tokenize(texts).numpy().astype(np.int32)\n\n        onnx_input_text = {self.textual_onnx_session.get_inputs()[0].name: texts}\n        embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n\n        response = ClipEmbeddingResponse(\n            embeddings=embeddings.tolist(), time=perf_counter() - t1\n        )\n\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\"textual.onnx\", \"visual.onnx\"]\n\n    def infer(self, request: ClipInferenceRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Routes the request to the appropriate inference function.\n\n        Args:\n            request (ClipInferenceRequest): The request object containing the inference details.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        if isinstance(request, ClipImageEmbeddingRequest):\n            infer_func = self.embed_image\n        elif isinstance(request, ClipTextEmbeddingRequest):\n            infer_func = self.embed_text\n        elif isinstance(request, ClipCompareRequest):\n            infer_func = self.compare\n        else:\n            raise ValueError(\n                f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n            )\n        return infer_func(request)\n\n    def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n\"\"\"Preprocesses an inference request image.\n\n        Args:\n            image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n        Returns:\n            np.ndarray: A numpy array of the preprocessed image pixel data.\n        \"\"\"\n        pil_image = self.load_image(image.type, image.value)\n        preprocessed_image = self.clip_preprocess(pil_image)\n\n        img_in = np.expand_dims(preprocessed_image, axis=0)\n\n        return img_in.astype(np.float32)\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the ClipOnnxRoboflowCoreModel with the given arguments and keyword arguments.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initializes the ClipOnnxRoboflowCoreModel with the given arguments and keyword arguments.\"\"\"\n\n    t1 = perf_counter()\n    super().__init__(*args, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n    self.visual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"visual.onnx\"),\n        providers=[\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                },\n            ),\n            \"CUDAExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n\n    self.textual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"textual.onnx\"),\n        providers=[\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                },\n            ),\n            \"CUDAExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n    self.clip_preprocess = _transform(self.resolution)\n    self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.compare","title":"<code>compare(request)</code>","text":"<p>Compares the subject with the prompt using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipCompareRequest</code> <p>The request object containing the subject and prompt.</p> required <p>Returns:</p> Name Type Description <code>ClipCompareResponse</code> <code>ClipCompareResponse</code> <p>The response object containing the similarity score.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def compare(self, request: ClipCompareRequest) -&gt; ClipCompareResponse:\n\"\"\"Compares the subject with the prompt using the Clip model.\n\n    Args:\n        request (ClipCompareRequest): The request object containing the subject and prompt.\n\n    Returns:\n        ClipCompareResponse: The response object containing the similarity score.\n    \"\"\"\n    t1 = perf_counter()\n    if request.subject_type == \"image\":\n        subject_embed_request = ClipImageEmbeddingRequest(image=request.subject)\n        subject_embeddings = self.embed_image(subject_embed_request).embeddings[0]\n    elif request.subject_type == \"text\":\n        subject_embed_request = ClipTextEmbeddingRequest(text=request.subject)\n        subject_embeddings = self.embed_text(subject_embed_request).embeddings[0]\n    else:\n        raise ValueError(\n            \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n        )\n\n    if isinstance(request.prompt, dict):\n        prompt_keys = request.prompt.keys()\n        prompt = [request.prompt[k] for k in prompt_keys]\n        prompt_obj = \"dict\"\n    else:\n        prompt = request.prompt\n        if not isinstance(prompt, list):\n            prompt = [prompt]\n        prompt_obj = \"list\"\n\n    if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n        raise ValueError(\n            f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n        )\n\n    if request.prompt_type == \"image\":\n        prompt_embed_request = ClipImageEmbeddingRequest(image=prompt)\n        prompt_embeddings = self.embed_image(prompt_embed_request).embeddings\n    elif request.prompt_type == \"text\":\n        prompt_embed_request = ClipTextEmbeddingRequest(text=prompt)\n        prompt_embeddings = self.embed_text(prompt_embed_request).embeddings\n    else:\n        raise ValueError(\n            \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n        )\n\n    similarities = [\n        cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n    ]\n\n    if prompt_obj == \"dict\":\n        similarities = dict(zip(prompt_keys, similarities))\n\n    response = ClipCompareResponse(\n        similarity=similarities, time=perf_counter() - t1\n    )\n\n    return response\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.embed_image","title":"<code>embed_image(request)</code>","text":"<p>Embeds an image using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipImageEmbeddingRequest</code> <p>The request object containing the image.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def embed_image(self, request: ClipImageEmbeddingRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Embeds an image using the Clip model.\n\n    Args:\n        request (ClipImageEmbeddingRequest): The request object containing the image.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(request.image, list):\n        if len(request.image) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n        imgs = [self.preproc_image(i) for i in request.image]\n        img_in = np.concatenate(imgs, axis=0)\n    else:\n        img_in = self.preproc_image(request.image)\n\n    onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n    embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n    response = ClipEmbeddingResponse(\n        embeddings=embeddings.tolist(), time=perf_counter() - t1\n    )\n\n    return response\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.embed_text","title":"<code>embed_text(request)</code>","text":"<p>Embeds a text using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipTextEmbeddingRequest</code> <p>The request object containing the text.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def embed_text(self, request: ClipTextEmbeddingRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Embeds a text using the Clip model.\n\n    Args:\n        request (ClipTextEmbeddingRequest): The request object containing the text.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(request.text, list):\n        if len(request.text) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of text strings that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        texts = request.text\n    else:\n        texts = [request.text]\n\n    texts = tokenize(texts).numpy().astype(np.int32)\n\n    onnx_input_text = {self.textual_onnx_session.get_inputs()[0].name: texts}\n    embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n\n    response = ClipEmbeddingResponse(\n        embeddings=embeddings.tolist(), time=perf_counter() - t1\n    )\n\n    return response\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\"textual.onnx\", \"visual.onnx\"]\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.infer","title":"<code>infer(request)</code>","text":"<p>Routes the request to the appropriate inference function.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipInferenceRequest</code> <p>The request object containing the inference details.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def infer(self, request: ClipInferenceRequest) -&gt; ClipEmbeddingResponse:\n\"\"\"Routes the request to the appropriate inference function.\n\n    Args:\n        request (ClipInferenceRequest): The request object containing the inference details.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    if isinstance(request, ClipImageEmbeddingRequest):\n        infer_func = self.embed_image\n    elif isinstance(request, ClipTextEmbeddingRequest):\n        infer_func = self.embed_text\n    elif isinstance(request, ClipCompareRequest):\n        infer_func = self.compare\n    else:\n        raise ValueError(\n            f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n        )\n    return infer_func(request)\n</code></pre>"},{"location":"library/models/clip/#inference.models.clip.clip.ClipOnnxRoboflowCoreModel.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an inference request image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The object containing information necessary to load the image for inference.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: A numpy array of the preprocessed image pixel data.</p> Source code in <code>inference/models/clip/clip.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n\"\"\"Preprocesses an inference request image.\n\n    Args:\n        image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n    Returns:\n        np.ndarray: A numpy array of the preprocessed image pixel data.\n    \"\"\"\n    pil_image = self.load_image(image.type, image.value)\n    preprocessed_image = self.clip_preprocess(pil_image)\n\n    img_in = np.expand_dims(preprocessed_image, axis=0)\n\n    return img_in.astype(np.float32)\n</code></pre>"},{"location":"library/models/segment_anything/","title":"Segment Anything","text":"<p>         Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnythingRoboflowCoreModel class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> <code>low_res_logits_cache</code> <p>Cache for low resolution logits.</p> <code>segmentation_cache_keys</code> <p>Keys for the segmentation cache.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>class SegmentAnythingRoboflowCoreModel(RoboflowCoreModel):\n\"\"\"SegmentAnythingRoboflowCoreModel class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n        low_res_logits_cache: Cache for low resolution logits.\n        segmentation_cache_keys: Keys for the segmentation cache.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initializes the SegmentAnythingRoboflowCoreModel.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.sam = sam_model_registry[self.version_id](\n            checkpoint=self.cache_file(\"encoder.pth\")\n        )\n        self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.predictor = SamPredictor(self.sam)\n        self.ort_session = onnxruntime.InferenceSession(\n            self.cache_file(\"decoder.onnx\"),\n            providers=[\n                \"CUDAExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n\n        self.low_res_logits_cache = {}\n        self.segmentation_cache_keys = []\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"encoder.pth\", \"decoder.onnx\"]\n\n    def embed_image_(self, request: SamEmbeddingRequest):\n\"\"\"Embeds an image.\n\n        Args:\n            request (SamEmbeddingRequest): The embedding request.\n\n        Returns:\n            Tuple: The embedding and the shape of the image.\n        \"\"\"\n        if request.image_id and request.image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[request.image_id],\n                self.image_size_cache[request.image_id],\n            )\n        img_in = self.preproc_image(request.image)\n        self.predictor.set_image(img_in)\n        embedding = self.predictor.get_image_embedding().cpu().numpy()\n        if request.image_id:\n            self.embedding_cache[request.image_id] = embedding\n            self.image_size_cache[request.image_id] = img_in.shape[:2]\n            self.embedding_cache_keys.append(request.image_id)\n            if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.embedding_cache_keys.pop(0)\n                del self.embedding_cache[cache_key]\n                del self.image_size_cache[cache_key]\n        return (embedding, img_in.shape[:2])\n\n    def embed_image(self, request: SamEmbeddingRequest):\n\"\"\"Embeds an image and returns the response.\n\n        Args:\n            request (SamEmbeddingRequest): The embedding request.\n\n        Returns:\n            SamEmbeddingResponse: The embedding response.\n        \"\"\"\n        t1 = perf_counter()\n        embedding, _ = self.embed_image_(request)\n        inference_time = perf_counter() - t1\n        if request.format == \"json\":\n            return SamEmbeddingResponse(\n                embeddings=embedding.tolist(), time=inference_time\n            )\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.save(binary_vector, embedding)\n            binary_vector.seek(0)\n            return SamEmbeddingResponse(\n                embeddings=binary_vector.getvalue(), time=inference_time\n            )\n\n    def infer(self, request: SamInferenceRequest):\n\"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        if isinstance(request, SamEmbeddingRequest):\n            return self.embed_image(request)\n        elif isinstance(request, SamSegmentationRequest):\n            return self.segment_image(request)\n\n    def preproc_image(self, image: InferenceRequestImage):\n\"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        pil_image = self.load_image(image.type, image.value).convert(\"RGB\")\n        return np.array(pil_image)\n\n    def segment_image(self, request: SamSegmentationRequest):\n\"\"\"Segments an image.\n\n        Args:\n            request (SamSegmentationRequest): The segmentation request.\n\n        Returns:\n            SamSegmentationResponse: The segmentation response.\n        \"\"\"\n        t1 = perf_counter()\n        if not request.embeddings:\n            if not request.image and not request.image_id:\n                raise ValueError(\n                    \"Must provide either image, cached image_id, or embeddings\"\n                )\n            elif (\n                request.image_id\n                and not request.image\n                and request.image_id not in self.embedding_cache\n            ):\n                raise ValueError(\n                    f\"Image ID {request.image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding_request = SamEmbeddingRequest(\n                image=request.image, image_id=request.image_id\n            )\n            embedding, original_image_size = self.embed_image_(embedding_request)\n        else:\n            if not request.orig_im_size:\n                raise ValueError(\n                    \"Must provide original image size if providing embeddings\"\n                )\n            original_image_size = request.orig_im_size\n            if request.embeddings_format == \"json\":\n                embedding = np.array(request.embeddings)\n            elif request.embeddings_format == \"binary\":\n                embedding = np.load(BytesIO(request.embeddings))\n\n        point_coords = request.point_coords\n        point_coords.append([0, 0])\n        point_coords = np.array(point_coords, dtype=np.float32)\n        point_coords = np.expand_dims(point_coords, axis=0)\n        point_coords = self.predictor.transform.apply_coords(\n            point_coords,\n            original_image_size,\n        )\n\n        point_labels = request.point_labels\n        point_labels.append(-1)\n        point_labels = np.array(point_labels, dtype=np.float32)\n        point_labels = np.expand_dims(point_labels, axis=0)\n\n        if request.has_mask_input:\n            if (\n                request.image_id\n                and request.image_id in self.low_res_logits_cache\n                and request.use_mask_input_cache\n            ):\n                mask_input = self.low_res_logits_cache[request.image_id]\n            elif not request.mask_input and (\n                not request.image_id\n                or request.image_id not in self.low_res_logits_cache\n            ):\n                raise ValueError(\"Must provide either mask_input or cached image_id\")\n            else:\n                if request.mask_input_format == \"json\":\n                    polys = request.mask_input\n                    mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                    for i, poly in enumerate(polys):\n                        poly = ShapelyPolygon(poly)\n                        raster = rasterio.features.rasterize(\n                            [poly], out_shape=(256, 256)\n                        )\n                        mask_input[0, i, :, :] = raster\n                elif request.mask_input_format == \"binary\":\n                    binary_data = base64.b64decode(request.mask_input)\n                    mask_input = np.load(BytesIO(binary_data))\n        else:\n            mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n        ort_inputs = {\n            \"image_embeddings\": embedding.astype(np.float32),\n            \"point_coords\": point_coords.astype(np.float32),\n            \"point_labels\": point_labels,\n            \"mask_input\": mask_input.astype(np.float32),\n            \"has_mask_input\": np.zeros(1, dtype=np.float32)\n            if not request.has_mask_input\n            else np.ones(1, dtype=np.float32),\n            \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n        }\n        masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n        if request.image_id:\n            self.low_res_logits_cache[request.image_id] = low_res_logits\n            if request.image_id not in self.segmentation_cache_keys:\n                self.segmentation_cache_keys.append(request.image_id)\n            if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.segmentation_cache_keys.pop(0)\n                del self.low_res_logits_cache[cache_key]\n        masks = masks[0]\n        low_res_masks = low_res_logits[0]\n\n        if request.format == \"json\":\n            masks = masks &gt; self.predictor.model.mask_threshold\n            masks = mask2poly(masks, False)\n            low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n            low_res_masks = mask2poly(low_res_masks, False)\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.savez_compressed(binary_vector, masks=masks, low_res_masks=low_res_masks)\n            binary_vector.seek(0)\n            binary_data = binary_vector.getvalue()\n            return binary_data\n        else:\n            raise ValueError(f\"Invalid format {request.format}\")\n\n        response = SamSegmentationResponse(\n            masks=masks,\n            low_res_masks=low_res_masks,\n            time=perf_counter() - t1,\n        )\n        return response\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the SegmentAnythingRoboflowCoreModel.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initializes the SegmentAnythingRoboflowCoreModel.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.sam = sam_model_registry[self.version_id](\n        checkpoint=self.cache_file(\"encoder.pth\")\n    )\n    self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.predictor = SamPredictor(self.sam)\n    self.ort_session = onnxruntime.InferenceSession(\n        self.cache_file(\"decoder.onnx\"),\n        providers=[\n            \"CUDAExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n\n    self.low_res_logits_cache = {}\n    self.segmentation_cache_keys = []\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.embed_image","title":"<code>embed_image(request)</code>","text":"<p>Embeds an image and returns the response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamEmbeddingRequest</code> <p>The embedding request.</p> required <p>Returns:</p> Name Type Description <code>SamEmbeddingResponse</code> <p>The embedding response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def embed_image(self, request: SamEmbeddingRequest):\n\"\"\"Embeds an image and returns the response.\n\n    Args:\n        request (SamEmbeddingRequest): The embedding request.\n\n    Returns:\n        SamEmbeddingResponse: The embedding response.\n    \"\"\"\n    t1 = perf_counter()\n    embedding, _ = self.embed_image_(request)\n    inference_time = perf_counter() - t1\n    if request.format == \"json\":\n        return SamEmbeddingResponse(\n            embeddings=embedding.tolist(), time=inference_time\n        )\n    elif request.format == \"binary\":\n        binary_vector = BytesIO()\n        np.save(binary_vector, embedding)\n        binary_vector.seek(0)\n        return SamEmbeddingResponse(\n            embeddings=binary_vector.getvalue(), time=inference_time\n        )\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.embed_image_","title":"<code>embed_image_(request)</code>","text":"<p>Embeds an image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamEmbeddingRequest</code> <p>The embedding request.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <p>The embedding and the shape of the image.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def embed_image_(self, request: SamEmbeddingRequest):\n\"\"\"Embeds an image.\n\n    Args:\n        request (SamEmbeddingRequest): The embedding request.\n\n    Returns:\n        Tuple: The embedding and the shape of the image.\n    \"\"\"\n    if request.image_id and request.image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[request.image_id],\n            self.image_size_cache[request.image_id],\n        )\n    img_in = self.preproc_image(request.image)\n    self.predictor.set_image(img_in)\n    embedding = self.predictor.get_image_embedding().cpu().numpy()\n    if request.image_id:\n        self.embedding_cache[request.image_id] = embedding\n        self.image_size_cache[request.image_id] = img_in.shape[:2]\n        self.embedding_cache_keys.append(request.image_id)\n        if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n    return (embedding, img_in.shape[:2])\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"encoder.pth\", \"decoder.onnx\"]\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.infer","title":"<code>infer(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def infer(self, request: SamInferenceRequest):\n\"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    if isinstance(request, SamEmbeddingRequest):\n        return self.embed_image(request)\n    elif isinstance(request, SamSegmentationRequest):\n        return self.segment_image(request)\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n\"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    pil_image = self.load_image(image.type, image.value).convert(\"RGB\")\n    return np.array(pil_image)\n</code></pre>"},{"location":"library/models/segment_anything/#inference.models.sam.segment_anything.SegmentAnythingRoboflowCoreModel.segment_image","title":"<code>segment_image(request)</code>","text":"<p>Segments an image.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamSegmentationRequest</code> <p>The segmentation request.</p> required <p>Returns:</p> Name Type Description <code>SamSegmentationResponse</code> <p>The segmentation response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def segment_image(self, request: SamSegmentationRequest):\n\"\"\"Segments an image.\n\n    Args:\n        request (SamSegmentationRequest): The segmentation request.\n\n    Returns:\n        SamSegmentationResponse: The segmentation response.\n    \"\"\"\n    t1 = perf_counter()\n    if not request.embeddings:\n        if not request.image and not request.image_id:\n            raise ValueError(\n                \"Must provide either image, cached image_id, or embeddings\"\n            )\n        elif (\n            request.image_id\n            and not request.image\n            and request.image_id not in self.embedding_cache\n        ):\n            raise ValueError(\n                f\"Image ID {request.image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding_request = SamEmbeddingRequest(\n            image=request.image, image_id=request.image_id\n        )\n        embedding, original_image_size = self.embed_image_(embedding_request)\n    else:\n        if not request.orig_im_size:\n            raise ValueError(\n                \"Must provide original image size if providing embeddings\"\n            )\n        original_image_size = request.orig_im_size\n        if request.embeddings_format == \"json\":\n            embedding = np.array(request.embeddings)\n        elif request.embeddings_format == \"binary\":\n            embedding = np.load(BytesIO(request.embeddings))\n\n    point_coords = request.point_coords\n    point_coords.append([0, 0])\n    point_coords = np.array(point_coords, dtype=np.float32)\n    point_coords = np.expand_dims(point_coords, axis=0)\n    point_coords = self.predictor.transform.apply_coords(\n        point_coords,\n        original_image_size,\n    )\n\n    point_labels = request.point_labels\n    point_labels.append(-1)\n    point_labels = np.array(point_labels, dtype=np.float32)\n    point_labels = np.expand_dims(point_labels, axis=0)\n\n    if request.has_mask_input:\n        if (\n            request.image_id\n            and request.image_id in self.low_res_logits_cache\n            and request.use_mask_input_cache\n        ):\n            mask_input = self.low_res_logits_cache[request.image_id]\n        elif not request.mask_input and (\n            not request.image_id\n            or request.image_id not in self.low_res_logits_cache\n        ):\n            raise ValueError(\"Must provide either mask_input or cached image_id\")\n        else:\n            if request.mask_input_format == \"json\":\n                polys = request.mask_input\n                mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                for i, poly in enumerate(polys):\n                    poly = ShapelyPolygon(poly)\n                    raster = rasterio.features.rasterize(\n                        [poly], out_shape=(256, 256)\n                    )\n                    mask_input[0, i, :, :] = raster\n            elif request.mask_input_format == \"binary\":\n                binary_data = base64.b64decode(request.mask_input)\n                mask_input = np.load(BytesIO(binary_data))\n    else:\n        mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n    ort_inputs = {\n        \"image_embeddings\": embedding.astype(np.float32),\n        \"point_coords\": point_coords.astype(np.float32),\n        \"point_labels\": point_labels,\n        \"mask_input\": mask_input.astype(np.float32),\n        \"has_mask_input\": np.zeros(1, dtype=np.float32)\n        if not request.has_mask_input\n        else np.ones(1, dtype=np.float32),\n        \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n    }\n    masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n    if request.image_id:\n        self.low_res_logits_cache[request.image_id] = low_res_logits\n        if request.image_id not in self.segmentation_cache_keys:\n            self.segmentation_cache_keys.append(request.image_id)\n        if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.segmentation_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n    masks = masks[0]\n    low_res_masks = low_res_logits[0]\n\n    if request.format == \"json\":\n        masks = masks &gt; self.predictor.model.mask_threshold\n        masks = mask2poly(masks, False)\n        low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n        low_res_masks = mask2poly(low_res_masks, False)\n    elif request.format == \"binary\":\n        binary_vector = BytesIO()\n        np.savez_compressed(binary_vector, masks=masks, low_res_masks=low_res_masks)\n        binary_vector.seek(0)\n        binary_data = binary_vector.getvalue()\n        return binary_data\n    else:\n        raise ValueError(f\"Invalid format {request.format}\")\n\n    response = SamSegmentationResponse(\n        masks=masks,\n        low_res_masks=low_res_masks,\n        time=perf_counter() - t1,\n    )\n    return response\n</code></pre>"},{"location":"library/models/vit_classification/","title":"ViT Classification","text":"<p>         Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code>, <code>ClassificationMixin</code></p> <p>VitClassificationOnnxRoboflowInferenceModel handles classification inference for Vision Transformer (ViT) models using ONNX.</p> Inherits <p>ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference. ClassificationMixin: Mixin class providing classification-specific methods.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>A flag that specifies if the model should handle multiclass classification.</p> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>class VitClassificationOnnxRoboflowInferenceModel(\n    ClassificationBaseOnnxRoboflowInferenceModel, ClassificationMixin\n):\n\"\"\"VitClassificationOnnxRoboflowInferenceModel handles classification inference\n    for Vision Transformer (ViT) models using ONNX.\n\n    Inherits:\n        ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference.\n        ClassificationMixin: Mixin class providing classification-specific methods.\n\n    Attributes:\n        multiclass (bool): A flag that specifies if the model should handle multiclass classification.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initializes the VitClassificationOnnxRoboflowInferenceModel instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Determines the weights file to be used based on the availability of AWS keys.\n\n        If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'.\n        Otherwise, it returns the path to 'best.onnx'.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n            return \"weights.onnx\"\n        else:\n            return \"best.onnx\"\n</code></pre>"},{"location":"library/models/vit_classification/#inference.models.vit.vit_classification.VitClassificationOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Determines the weights file to be used based on the availability of AWS keys.</p> <p>If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'. Otherwise, it returns the path to 'best.onnx'.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"library/models/vit_classification/#inference.models.vit.vit_classification.VitClassificationOnnxRoboflowInferenceModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the VitClassificationOnnxRoboflowInferenceModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initializes the VitClassificationOnnxRoboflowInferenceModel instance.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"library/models/yoloact_segmentation/","title":"YOLO-ACT Instance Segmentation","text":"<p>         Bases: <code>OnnxRoboflowInferenceModel</code>, <code>InstanceSegmentationMixin</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method)</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>class YOLACTInstanceSegmentationOnnxRoboflowInferenceModel(\n    OnnxRoboflowInferenceModel, InstanceSegmentationMixin\n):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method)\"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def infer(\n        self, request: InstanceSegmentationInferenceRequest\n    ) -&gt; Union[\n        List[InstanceSegmentationInferenceResponse],\n        InstanceSegmentationInferenceResponse,\n    ]:\n\"\"\"Takes an instance segmentation inference request, preprocesses all images, runs inference on all images, and returns the postprocessed detections in the form of inference response objects.\n\n        Args:\n            request (InstanceSegmentationInferenceRequest): A request containing 1 to N inference image objects and other inference parameters (confidence, iou threshold, etc.)\n\n        Returns:\n            Union[List[InstanceSegmentationInferenceResponse], InstanceSegmentationInferenceResponse]: One to N inference response objects based on the number of inference request images in the inference request object, each inference response object contains a list of predictions.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(request.image, list):\n            imgs_with_dims = [self.preproc_image(i) for i in request.image]\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n            unwrap = False\n        else:\n            img_in, img_dims = self.preproc_image(request.image)\n            img_dims = [img_dims]\n            unwrap = True\n\n        # IN BGR order (for some reason)\n        mean = (103.94, 116.78, 123.68)\n        std = (57.38, 57.12, 58.40)\n\n        img_in = img_in.astype(np.float32)\n\n        # Our channels are RGB, so apply mean and std accordingly\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[2]) / std[2]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[0]) / std[0]\n\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n\n        loc_data = np.float32(predictions[0])\n        conf_data = np.float32(predictions[1])\n        mask_data = np.float32(predictions[2])\n        prior_data = np.float32(predictions[3])\n        proto_data = np.float32(predictions[4])\n\n        batch_size = loc_data.shape[0]\n        num_priors = prior_data.shape[0]\n\n        boxes = np.zeros((batch_size, num_priors, 4))\n        for batch_idx in range(batch_size):\n            boxes[batch_idx, :, :] = self.decode_predicted_bboxes(\n                loc_data[batch_idx], prior_data\n            )\n\n        conf_preds = np.reshape(\n            conf_data, (batch_size, num_priors, self.num_classes + 1)\n        )\n        class_confs = conf_preds[:, :, 1:]  # remove background class\n        box_confs = np.expand_dims(\n            np.max(class_confs, axis=2), 2\n        )  # get max conf for each box\n\n        predictions = np.concatenate((boxes, box_confs, class_confs, mask_data), axis=2)\n\n        predictions[:, :, 0] *= img_in.shape[2]\n        predictions[:, :, 1] *= img_in.shape[3]\n        predictions[:, :, 2] *= img_in.shape[2]\n        predictions[:, :, 3] *= img_in.shape[3]\n\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=request.confidence,\n            iou_thresh=request.iou_threshold,\n            class_agnostic=request.class_agnostic_nms,\n            max_detections=request.max_detections,\n            max_candidate_detections=request.max_candidates,\n            num_masks=32,\n            box_format=\"xyxy\",\n        )\n        predictions = np.array(predictions)\n\n        batch_preds = []\n        for batch_idx, img_dim in zip(range(batch_size), img_dims):\n            boxes = predictions[batch_idx, :, :4]\n            scores = predictions[batch_idx, :, 4]\n            classes = predictions[batch_idx, :, 6]\n            masks = predictions[batch_idx, :, 7:]\n            proto = proto_data[batch_idx]\n            decoded_masks = self.decode_masks(boxes, masks, proto, img_in.shape[2:])\n            polys = mask2poly(decoded_masks)\n            infer_shape = (self.img_size_w, self.img_size_h)\n            boxes = postprocess_predictions(\n                [boxes], infer_shape, [img_dim], self.preproc, self.resize_method\n            )[0]\n            polys = scale_polys(\n                img_in.shape[2:],\n                polys,\n                img_dim,\n                self.preproc,\n                resize_method=self.resize_method,\n            )\n            preds = []\n            for i, (box, poly, score, cls) in enumerate(\n                zip(boxes, polys, scores, classes)\n            ):\n                confidence = float(score)\n                class_name = self.class_names[int(cls)]\n                points = [{\"x\": round(x, 1), \"y\": round(y, 1)} for (x, y) in poly]\n                pred = {\n                    \"x\": round((box[2] + box[0]) / 2, 1),\n                    \"y\": round((box[3] + box[1]) / 2, 1),\n                    \"width\": int(box[2] - box[0]),\n                    \"height\": int(box[3] - box[1]),\n                    \"class\": class_name,\n                    \"confidence\": round(confidence, 3),\n                    \"points\": points,\n                }\n                if not request.class_filter or class_name in request.class_filter:\n                    preds.append(pred)\n            batch_preds.append(preds)\n\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[InstanceSegmentationPrediction(**p) for p in batch_pred],\n                time=perf_counter() - t1,\n                image=InferenceResponseImage(\n                    width=img_dims[i][1], height=img_dims[i][0]\n                ),\n            )\n            for i, batch_pred in enumerate(batch_preds)\n        ]\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if unwrap:\n            responses = responses[0]\n        return responses\n\n    def decode_masks(self, boxes, masks, proto, img_dim):\n\"\"\"Decodes the masks from the given parameters.\n\n        Args:\n            boxes (np.array): Bounding boxes.\n            masks (np.array): Masks.\n            proto (np.array): Proto data.\n            img_dim (tuple): Image dimensions.\n\n        Returns:\n            np.array: Decoded masks.\n        \"\"\"\n        ret_mask = np.matmul(proto, np.transpose(masks))\n        ret_mask = 1 / (1 + np.exp(-ret_mask))\n        w, h, _ = ret_mask.shape\n        gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n        pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n        top, left = int(pad[1]), int(pad[0])  # y, x\n        bottom, right = int(h - pad[1]), int(w - pad[0])\n        ret_mask = np.transpose(ret_mask, (2, 0, 1))\n        ret_mask = ret_mask[:, top:bottom, left:right]\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=0)\n        ret_mask = ret_mask.transpose((1, 2, 0))\n        ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=2)\n        ret_mask = ret_mask.transpose((2, 0, 1))\n        ret_mask = crop_mask(ret_mask, boxes)  # CHW\n        ret_mask[ret_mask &lt; 0.5] = 0\n\n        return ret_mask\n\n    def decode_predicted_bboxes(self, loc, priors):\n\"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n        Args:\n            loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n            priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n        Returns:\n            np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n        \"\"\"\n\n        variances = [0.1, 0.2]\n\n        boxes = np.concatenate(\n            [\n                priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n            ],\n            1,\n        )\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n\n        return boxes\n</code></pre>"},{"location":"library/models/yoloact_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACTInstanceSegmentationOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"library/models/yoloact_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACTInstanceSegmentationOnnxRoboflowInferenceModel.decode_masks","title":"<code>decode_masks(boxes, masks, proto, img_dim)</code>","text":"<p>Decodes the masks from the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>np.array</code> <p>Bounding boxes.</p> required <code>masks</code> <code>np.array</code> <p>Masks.</p> required <code>proto</code> <code>np.array</code> <p>Proto data.</p> required <code>img_dim</code> <code>tuple</code> <p>Image dimensions.</p> required <p>Returns:</p> Type Description <p>np.array: Decoded masks.</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_masks(self, boxes, masks, proto, img_dim):\n\"\"\"Decodes the masks from the given parameters.\n\n    Args:\n        boxes (np.array): Bounding boxes.\n        masks (np.array): Masks.\n        proto (np.array): Proto data.\n        img_dim (tuple): Image dimensions.\n\n    Returns:\n        np.array: Decoded masks.\n    \"\"\"\n    ret_mask = np.matmul(proto, np.transpose(masks))\n    ret_mask = 1 / (1 + np.exp(-ret_mask))\n    w, h, _ = ret_mask.shape\n    gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n    pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n    top, left = int(pad[1]), int(pad[0])  # y, x\n    bottom, right = int(h - pad[1]), int(w - pad[0])\n    ret_mask = np.transpose(ret_mask, (2, 0, 1))\n    ret_mask = ret_mask[:, top:bottom, left:right]\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=0)\n    ret_mask = ret_mask.transpose((1, 2, 0))\n    ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=2)\n    ret_mask = ret_mask.transpose((2, 0, 1))\n    ret_mask = crop_mask(ret_mask, boxes)  # CHW\n    ret_mask[ret_mask &lt; 0.5] = 0\n\n    return ret_mask\n</code></pre>"},{"location":"library/models/yoloact_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACTInstanceSegmentationOnnxRoboflowInferenceModel.decode_predicted_bboxes","title":"<code>decode_predicted_bboxes(loc, priors)</code>","text":"<p>Decode predicted bounding box coordinates using the scheme employed by Yolov2.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>np.array</code> <p>The predicted bounding boxes of size [num_priors, 4].</p> required <code>priors</code> <code>np.array</code> <p>The prior box coordinates with size [num_priors, 4].</p> required <p>Returns:</p> Type Description <p>np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_predicted_bboxes(self, loc, priors):\n\"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n    Args:\n        loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n        priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n    Returns:\n        np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n    \"\"\"\n\n    variances = [0.1, 0.2]\n\n    boxes = np.concatenate(\n        [\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n        ],\n        1,\n    )\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n\n    return boxes\n</code></pre>"},{"location":"library/models/yoloact_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACTInstanceSegmentationOnnxRoboflowInferenceModel.infer","title":"<code>infer(request)</code>","text":"<p>Takes an instance segmentation inference request, preprocesses all images, runs inference on all images, and returns the postprocessed detections in the form of inference response objects.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstanceSegmentationInferenceRequest</code> <p>A request containing 1 to N inference image objects and other inference parameters (confidence, iou threshold, etc.)</p> required <p>Returns:</p> Type Description <code>Union[List[InstanceSegmentationInferenceResponse], InstanceSegmentationInferenceResponse]</code> <p>Union[List[InstanceSegmentationInferenceResponse], InstanceSegmentationInferenceResponse]: One to N inference response objects based on the number of inference request images in the inference request object, each inference response object contains a list of predictions.</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def infer(\n    self, request: InstanceSegmentationInferenceRequest\n) -&gt; Union[\n    List[InstanceSegmentationInferenceResponse],\n    InstanceSegmentationInferenceResponse,\n]:\n\"\"\"Takes an instance segmentation inference request, preprocesses all images, runs inference on all images, and returns the postprocessed detections in the form of inference response objects.\n\n    Args:\n        request (InstanceSegmentationInferenceRequest): A request containing 1 to N inference image objects and other inference parameters (confidence, iou threshold, etc.)\n\n    Returns:\n        Union[List[InstanceSegmentationInferenceResponse], InstanceSegmentationInferenceResponse]: One to N inference response objects based on the number of inference request images in the inference request object, each inference response object contains a list of predictions.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(request.image, list):\n        imgs_with_dims = [self.preproc_image(i) for i in request.image]\n        imgs, img_dims = zip(*imgs_with_dims)\n        img_in = np.concatenate(imgs, axis=0)\n        unwrap = False\n    else:\n        img_in, img_dims = self.preproc_image(request.image)\n        img_dims = [img_dims]\n        unwrap = True\n\n    # IN BGR order (for some reason)\n    mean = (103.94, 116.78, 123.68)\n    std = (57.38, 57.12, 58.40)\n\n    img_in = img_in.astype(np.float32)\n\n    # Our channels are RGB, so apply mean and std accordingly\n    img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[2]) / std[2]\n    img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n    img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[0]) / std[0]\n\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n\n    loc_data = np.float32(predictions[0])\n    conf_data = np.float32(predictions[1])\n    mask_data = np.float32(predictions[2])\n    prior_data = np.float32(predictions[3])\n    proto_data = np.float32(predictions[4])\n\n    batch_size = loc_data.shape[0]\n    num_priors = prior_data.shape[0]\n\n    boxes = np.zeros((batch_size, num_priors, 4))\n    for batch_idx in range(batch_size):\n        boxes[batch_idx, :, :] = self.decode_predicted_bboxes(\n            loc_data[batch_idx], prior_data\n        )\n\n    conf_preds = np.reshape(\n        conf_data, (batch_size, num_priors, self.num_classes + 1)\n    )\n    class_confs = conf_preds[:, :, 1:]  # remove background class\n    box_confs = np.expand_dims(\n        np.max(class_confs, axis=2), 2\n    )  # get max conf for each box\n\n    predictions = np.concatenate((boxes, box_confs, class_confs, mask_data), axis=2)\n\n    predictions[:, :, 0] *= img_in.shape[2]\n    predictions[:, :, 1] *= img_in.shape[3]\n    predictions[:, :, 2] *= img_in.shape[2]\n    predictions[:, :, 3] *= img_in.shape[3]\n\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=request.confidence,\n        iou_thresh=request.iou_threshold,\n        class_agnostic=request.class_agnostic_nms,\n        max_detections=request.max_detections,\n        max_candidate_detections=request.max_candidates,\n        num_masks=32,\n        box_format=\"xyxy\",\n    )\n    predictions = np.array(predictions)\n\n    batch_preds = []\n    for batch_idx, img_dim in zip(range(batch_size), img_dims):\n        boxes = predictions[batch_idx, :, :4]\n        scores = predictions[batch_idx, :, 4]\n        classes = predictions[batch_idx, :, 6]\n        masks = predictions[batch_idx, :, 7:]\n        proto = proto_data[batch_idx]\n        decoded_masks = self.decode_masks(boxes, masks, proto, img_in.shape[2:])\n        polys = mask2poly(decoded_masks)\n        infer_shape = (self.img_size_w, self.img_size_h)\n        boxes = postprocess_predictions(\n            [boxes], infer_shape, [img_dim], self.preproc, self.resize_method\n        )[0]\n        polys = scale_polys(\n            img_in.shape[2:],\n            polys,\n            img_dim,\n            self.preproc,\n            resize_method=self.resize_method,\n        )\n        preds = []\n        for i, (box, poly, score, cls) in enumerate(\n            zip(boxes, polys, scores, classes)\n        ):\n            confidence = float(score)\n            class_name = self.class_names[int(cls)]\n            points = [{\"x\": round(x, 1), \"y\": round(y, 1)} for (x, y) in poly]\n            pred = {\n                \"x\": round((box[2] + box[0]) / 2, 1),\n                \"y\": round((box[3] + box[1]) / 2, 1),\n                \"width\": int(box[2] - box[0]),\n                \"height\": int(box[3] - box[1]),\n                \"class\": class_name,\n                \"confidence\": round(confidence, 3),\n                \"points\": points,\n            }\n            if not request.class_filter or class_name in request.class_filter:\n                preds.append(pred)\n        batch_preds.append(preds)\n\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[InstanceSegmentationPrediction(**p) for p in batch_pred],\n            time=perf_counter() - t1,\n            image=InferenceResponseImage(\n                width=img_dims[i][1], height=img_dims[i][0]\n            ),\n        )\n        for i, batch_pred in enumerate(batch_preds)\n    ]\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if unwrap:\n        responses = responses[0]\n    return responses\n</code></pre>"},{"location":"library/models/yolov5/","title":"YOLOv5 Object Detection","text":"<p>         Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code>, <code>ObjectDetectionMixin</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>class YOLOv5ObjectDetectionOnnxRoboflowInferenceModel(\n    ObjectDetectionBaseOnnxRoboflowInferenceModel, ObjectDetectionMixin\n):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray) -&gt; np.ndarray:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            np.ndarray: NumPy array representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        return predictions\n</code></pre>"},{"location":"library/models/yolov5/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetectionOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"library/models/yolov5/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetectionOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>np.ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray) -&gt; np.ndarray:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        np.ndarray: NumPy array representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    return predictions\n</code></pre>"},{"location":"library/models/yolov5_segmentation/","title":"YOLOv5 Instance Segmentation","text":"<p>         Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv5 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>class YOLOv5InstanceSegmentationOnnxRoboflowInferenceModel(\n    InstanceSegmentationBaseOnnxRoboflowInferenceModel\n):\n\"\"\"YOLOv5 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        return predictions[0], predictions[1]\n</code></pre>"},{"location":"library/models/yolov5_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentationOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"library/models/yolov5_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentationOnnxRoboflowInferenceModel.infer_onnx","title":"<code>infer_onnx(img_in)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>np.ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    return predictions[0], predictions[1]\n</code></pre>"},{"location":"library/models/yolov7_instance_segmentation/","title":"YOLOv7 Instance Segmentation","text":"<p>         Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv7 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv7 model with ONNX runtime.</p> Methods <p>infer_onnx: Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>class YOLOv7InstanceSegmentationOnnxRoboflowInferenceModel(\n    InstanceSegmentationBaseOnnxRoboflowInferenceModel\n):\n\"\"\"YOLOv7 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv7 model\n    with ONNX runtime.\n\n    Methods:\n        infer_onnx: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[4]\n        predictions = predictions[0]\n        return predictions, protos\n</code></pre>"},{"location":"library/models/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentationOnnxRoboflowInferenceModel.infer_onnx","title":"<code>infer_onnx(img_in)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>np.ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[4]\n    predictions = predictions[0]\n    return predictions, protos\n</code></pre>"},{"location":"library/models/yolov8/","title":"YOLOv8 Object Detection","text":"<p>         Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code>, <code>ObjectDetectionMixin</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Methods <p>predict: Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>class YOLOv8ObjectDetectionOnnxRoboflowInferenceModel(\n    ObjectDetectionBaseOnnxRoboflowInferenceModel, ObjectDetectionMixin\n):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray) -&gt; np.ndarray:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            np.ndarray: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return predictions\n</code></pre>"},{"location":"library/models/yolov8/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetectionOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"library/models/yolov8/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetectionOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>np.ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray) -&gt; np.ndarray:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        np.ndarray: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return predictions\n</code></pre>"},{"location":"library/models/yolov8_classification/","title":"YOLOv8 Classification","text":"<p>         Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code>, <code>ClassificationMixin</code></p> Source code in <code>inference/models/yolov8/yolov8_classification.py</code> <pre><code>class YOLOv8ClassificationOnnxRoboflowInferenceModel(\n    ClassificationBaseOnnxRoboflowInferenceModel, ClassificationMixin\n):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n        return \"weights.onnx\"\n</code></pre>"},{"location":"library/models/yolov8_segmentation/","title":"YOLOv8 Instance Segmentation","text":"<p>         Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv8 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Methods <p>infer_onnx: Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>class YOLOv8InstanceSegmentationOnnxRoboflowInferenceModel(\n    InstanceSegmentationBaseOnnxRoboflowInferenceModel\n):\n\"\"\"YOLOv8 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        infer_onnx: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[1]\n        predictions = predictions[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:-32]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        masks = predictions[:, :, -32:]\n        predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n        return predictions, protos\n</code></pre>"},{"location":"library/models/yolov8_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentationOnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"library/models/yolov8_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentationOnnxRoboflowInferenceModel.infer_onnx","title":"<code>infer_onnx(img_in)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>np.ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>def infer_onnx(self, img_in: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[1]\n    predictions = predictions[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:-32]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    masks = predictions[:, :, -32:]\n    predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n    return predictions, protos\n</code></pre>"},{"location":"quickstart/docker/","title":"Docker","text":""},{"location":"quickstart/docker/#setup","title":"Setup","text":"<p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment,  allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If  you haven't installed Docker yet, you can get it from Docker's official website.</p>"},{"location":"quickstart/docker/#pull","title":"Pull","text":"<p>If you don't wish to build the Docker image locally or prefer to use the official releases, you can directly pull the  pre-built images from the Docker Hub. These images are maintained by the Roboflow team and are optimized for various  hardware configurations.</p> <p>docker pull</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <p>Official Roboflow Inference Server Docker Image for x86 CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for ARM CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-arm-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-gpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU with TensorRT Runtime Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt-jetson\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt-jetson-5.1.1\n</code></pre>"},{"location":"quickstart/docker/#run","title":"Run","text":"<p>Once you have a Docker image (either built locally or pulled from Docker Hub), you can run the Roboflow Inference  Server in a container. </p> <p>docker run</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <pre><code>docker run --net=host \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -p 9001:9001 \\\nroboflow/roboflow-inference-server-arm-cpu:latest\n</code></pre> <pre><code>docker run --network=host --gpus=all \\\nroboflow/roboflow-inference-server-gpu:latest\n</code></pre> <pre><code>docker run --network=host --gpus=all \\\nroboflow/roboflow-inference-server-trt:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-trt-jetson:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-trt-jetson-5.1.1:latest\n</code></pre>"},{"location":"quickstart/docker/#build","title":"Build","text":"<p>To build a Docker image locally, first clone the Inference Server repository.</p> <pre><code>git clone git clone https://github.com/roboflow/inference\n</code></pre> <p>Choose a Dockerfile from the following options, depending on the hardware you want to run Inference Server on.</p> <p>docker build</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.arm.cpu \\\n-t roboflow/roboflow-inference-server-arm-cpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.gpu \\\n-t roboflow/roboflow-inference-server-gpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.trt \\\nroboflow/roboflow-inference-server-trt .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-trt-jetson .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.jetson.5.1.1 \\\n-t roboflow/roboflow-inference-server-trt-jetson-5.1.1 .\n</code></pre>"},{"location":"quickstart/http_inference/","title":"HTTP Inference","text":"<p>The Roboflow Inference Server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we walk through how to run inference on object detection, classification, and segmentation models using the Inference Server.</p> <p>Currently, the server is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>To run inference with the server, we will:</p> <ol> <li>Install the server</li> <li>Download a model for use on the server</li> <li>Run inference</li> </ol>"},{"location":"quickstart/http_inference/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using. Here are the available Docker containers:</p>"},{"location":"quickstart/http_inference/#arm-cpu","title":"ARM CPU","text":"<pre><code>sudo docker run -it --rm -p 9001:9001 roboflow/roboflow-inference-server-arm-cpu\n</code></pre>"},{"location":"quickstart/http_inference/#trt","title":"TRT","text":"<pre><code>sudo docker run --privileged --net=host --runtime=nvidia --mount source=roboflow,target=/cache -e NUM_WORKERS=1 roboflow/roboflow-inference-server-trt-jetson:latest\n</code></pre>"},{"location":"quickstart/http_inference/#gpu","title":"GPU","text":"<pre><code>[]\n</code></pre> <p>Run the relevant command for your device. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p> <p>Now we are ready to run inference!</p>"},{"location":"quickstart/http_inference/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>To run inference on a model, we will make a HTTP request to:</p> <pre><code>http://localhost:9001/{workspace_id}/{model_id}\n</code></pre> <p>To find your workspace and model IDs, refer to the Roboflow documentation.</p> <p>This route works for all supported task types: object detection, classification, and segmentation.</p> <p>To run inference, make a HTTP request to the route:</p> <pre><code>import requests\n\nworkspace_id = \"\"\nmodel_id = \"\"\nimage_url = \"\"\nconfidence = 0.75\napi_key = \"\"\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n    json=infer_object_detection_payload,\n)\n\npredictions = res.json()\n</code></pre> <p>This code will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>Above, set your workspace and model ID. Also configure your confidence and IoU threshold values as needed. If you are using classification, you can omit the IoU threshold value. You will also need to set your Roboflow API key. To learn how to retrieve your Roboflow API key, refer to the Roboflow API documentation.</p> <p>You can post either a URL, a base64-encoded image, or a pickled NumPy array to the server.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/library_inference/","title":"Python Inference","text":"<p>You can run inference on models in the Roboflow Inference Server directly using the <code>inference</code> Python package. This allows you to interface with models without running the Inference HTTP server.</p> <p>In this guide, we will show how to run inference on object detection, classification, and segmentation models using the <code>inference</code> package.</p> <p>Let's begin!</p>"},{"location":"quickstart/library_inference/#step-1-install-roboflow-inference","title":"Step 1: Install Roboflow Inference","text":"<p>First, we need to install Roboflow Inference. The command to install Roboflow Inference depends on the device on whihc you are running inference. Here are the available packages:</p> <ul> <li><code>inference[arm]</code>: ARM CPU</li> <li><code>inference[jetson]</code>: NVIDIA Jetson</li> <li><code>inference[trt]</code>: TensorRT devices</li> </ul> <p>Run the relevant command on your device. Once you have installed the Python package, you can start running inference.</p>"},{"location":"quickstart/library_inference/#step-2-choose-a-model","title":"Step 2: Choose a Model","text":"<p>At the moment, you can only run inference on models trained on Roboflow. Support for bringing your own models is being actively worked on. This guide will be updated when such support is available.</p>"}]}