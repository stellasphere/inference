{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#roboflow-inference","title":"Roboflow Inference \ud83d\udcbb","text":"<p>Roboflow Inference is an opinionated tool for running inference on state-of-the-art computer vision models. With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments.</p> <p>By using Roboflow Inference, you can:</p> <ol> <li>Write inference logic without having to configure environments or install dependencies.</li> <li>Deploy models to a range of devices and environments.</li> <li>Use state-of-the-art models with your own weights.</li> <li>Query models with HTTP.</li> <li>Scale up your inference server as your production needs grow with Docker.</li> </ol> <p>You can use Inference wherever you can use Docker: on CPUs, GPUs, TRT and more.</p> <p>You can run inference using the <code>inference</code> pip package, or using the HTTP server available by running a Roboflow Inference Docker containers.</p>"},{"location":"#installation","title":"Installation \ud83d\udee0\ufe0f","text":"<p>The Roboflow Inference runs in a Docker container. If you do not already have Docker installed on your computer, follow the official Docker installation instructions to install Docker.</p> <p>For all installation options, you will need a free Roboflow account and a Roboflow API key. You can learn how to retrieve your API key in the Roboflow API documentation.</p>"},{"location":"#install-using-pip","title":"Install Using Pip","text":"<p>To install the Inference using pip, run one of the following commands, depending on the device on which you want to run the Inference:</p> <pre><code>pip install inference[arm]\npip install inference[jetson]\npip install inference[trt]\n</code></pre>"},{"location":"#install-using-docker","title":"Install Using Docker","text":"<p>To install the Inference, first clone this repository:</p> <pre><code>git clone https://github.com/roboflow/inference\n</code></pre> <p>Then, choose a Dockerfile from the options below, depending on the environment in which you want to run the Inference:</p> <ul> <li><code>Dockerfile.onnx.arm.cpu</code>: ARM CPU</li> <li><code>Dockerfile.onnx.cpu</code>: CPU</li> <li><code>Dockerfile.onnx.gpu</code>: GPU</li> <li><code>Dockerfile.onnx.jetson</code>: NVIDIA Jetson</li> <li><code>Dockerfile.onnx.jetson.4.4.1</code>: NVIDIA Jetson 4.4.1</li> <li><code>Dockerfile.onnx.jetson.5.1.1</code>: NVIDIA Jetson 5.1.1</li> <li><code>Dockerfile.onnx.lambda</code>: AWS Lambda</li> <li><code>Dockerfile.onnx.trt</code>: TensorRT</li> </ul> <p>To install the server, first download and build the container:</p> <pre><code>docker build -f dockerfiles/[FILE_NAME] -t inference-server .\n</code></pre> <p>Replace <code>[FILE_NAME]</code> with the name of the Dockerfile that applies to the environment in which you want to run the Inference.</p> <p>Then, run the container:</p> <pre><code>docker run --gpus all --network=host -e PORT=9000 -e API_KEY=&lt;YOUR API KEY&gt; -v $(pwd)/cache:/cache inference-server\n</code></pre> <p>Substitute the API_KEY variable with your Roboflow API key. Learn how to retrieve your Roboflow API key.</p>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":"<p>API documentation are hosted by a running instance of the inference server at the <code>/docs</code> and <code>/redoc</code> endpoints. For an inference server running locally, see <code>https://localhost/docs</code> or <code>https://localhost/redoc</code>.</p>"},{"location":"#model-support","title":"Model Support \ud83d\uddbc\ufe0f","text":"<p>The Roboflow Inference supports the models listed below.</p> <p>You can also run any model hosted on Roboflow using the Inference.</p>"},{"location":"#classification","title":"Classification","text":"<ul> <li>ViT</li> <li>YOLOv8</li> <li>CLIP</li> </ul>"},{"location":"#object-detection","title":"Object Detection","text":"<ul> <li>YOLOv5</li> <li>YOLOv8</li> </ul>"},{"location":"#segmentation","title":"Segmentation","text":"<ul> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOACT</li> <li>Segment Anything</li> </ul>"},{"location":"#environment-variable-control","title":"Environment Variable Control \ud83c\udf10","text":"<p>Use these environment variables to control pingback and to Roboflow as well as other features; if you are running a Docker container, pass these into the docker run command.</p> ENV Variable Description PINGBACK_ENABLED Default is true; if set to the string \"false\", pingback messages are not sent back to Roboflow. PINGBACK_URL Default is <code>https://api.roboflow.com/pingback</code> PINGBACK_INTERVAL_SECONDS Frequency of sending pingback messages, default is 3600 seconds ROBOFLOW_SERVER_UUID If this is set, the ID of the process reported back to Roboflow's UI is the value of this environment variable. Omitting this causes the process (docker container) to generate a new UUID. DEVICE_ID This predates Pingback; if left unset, its default value is \"sample-device-id\" ENABLE_PROMETHEUS if set to any value, this will cause a /metrics endpoint to be created with some FastAPI metrics for Prometheus to scrape; not applicable to the lambda inference server"},{"location":"#community-resources","title":"Community Resources \ud83d\udcda","text":"<ul> <li>Roboflow Inference Documentation</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>[ ] Add support for more models</li> </ul>"},{"location":"#contributing","title":"Contributing \u2328\ufe0f","text":"<p>Thank you for your interest in contributing to the Roboflow Inference! You can learn more about how to start contributing in our contributing guide.</p>"},{"location":"#license","title":"License \ud83d\udcdd","text":"<p>The Roboflow Inference code, enclosed within <code>inference/core</code>, as well as the documentation in <code>docs/</code>, is licnsed under an Apache 2.0 license.</p> <p>The following models, accessible through, Roboflow Inference comes with their own licenses:</p> <ul> <li><code>inference/models/clip</code>: MIT.</li> <li><code>inference/models/sam</code>: Apache 2.0.</li> <li><code>inference/models/yolact</code>: MIT.</li> <li><code>inference/models/yolov5</code>: AGPL-3.0.</li> <li><code>inference/models/yolov7</code>: GPL-3.0.</li> <li><code>inference/models/yolov8</code>: AGPL-3.0.</li> </ul> <p>Roboflow Inference offers more features to Enterprise License holders, including:</p> <ol> <li>Running Inference on a cluster of multiple servers</li> <li>Handling auto-batched inference</li> <li>Device management</li> <li>Active learning</li> <li>Sub-license YOLOv5 and YOLOv8 models for enterprise use</li> <li>And more.</li> </ol> <p>To learn more about a Roboflow Inference Enterprise License, contact us.</p>"},{"location":"#build-the-documentation","title":"Build the Documentation","text":"<p>Roboflow Inference uses <code>mkdocs</code> and <code>mike</code> to offer versioned documentation. The project documentation is hosted at https://inference.roboflow.com</p> <p>To build the Inference documentation, first install the project development dependencies:</p> <pre><code>pip install -r dev-requirements.txt\n</code></pre> <p>To run the latest version of the documentation, run:</p> <pre><code>mike serve\n</code></pre> <p>Before a new release is published, a new version of the documentation should be built. To create a new version, run:</p> <pre><code>mike deploy &lt;version-number&gt;\n</code></pre>"},{"location":"#initial-benchmark-speed-results","title":"Initial Benchmark Speed Results \ud83d\udcc8","text":"<p>The Roboflow team has run initial benchmarks on the inference server. Our results are below.</p>"},{"location":"#legacy-trt","title":"Legacy TRT","text":"<ul> <li>Node Benchmark: 50FPS (36 frames in 0.72s)</li> <li>Python Benchmark: 22FPS (93FPS internal)</li> </ul>"},{"location":"#onnx-trt","title":"ONNX TRT","text":"<ul> <li>Node Benchmark: 63FPS (36 frames in 0.57s)</li> <li>Python Benchmark: 27FPS (53FPS internal)</li> </ul>"},{"location":"add_a_model/","title":"Add a Model to the Inference Server","text":"<p>The Roboflow Inference Server comes with several state-of-the-art models available for inference out of the box. If your model is not supported by the Inference Server, you can implement support for a custom model architecture.</p> <p>In this guide, we are going to walk through how to add support for a new model architecture to the Roboflow Inference Server.</p>"},{"location":"api/","title":"API Reference","text":"<p>The Roboflow Inference Server provides OpenAPI documentation at the <code>/docs</code> endpoint for use in development.</p> <p>Below is the OpenAPI specification for the Inference Server, rendered with Swagger.</p> <p></p>"},{"location":"contributing/","title":"Contributing to the Roboflow Inference Server \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to the Roboflow Inference Server!</p> <p>We welcome any contributions to help us improve the quality of <code>inference-server</code> and expand the range of supported models.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add support for running inference on a new model.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>The Inference Server provides a standard interface through which you can work with computer vision models. With Inference Server, you can use state-of-the-art models with your own weights without having to spend time installing dependencies, configuring environments, and writing inference code.</p> <p>We welcome contributions that add support for new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>You will need to add documentation for your model and link to it from the <code>inference-server</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"library/data_models/clip/clip_compare_request/","title":"CLIP Compare Request","text":""},{"location":"library/data_models/clip/clip_image_embedding_request/","title":"CLIP Image Embedding Request","text":""},{"location":"library/data_models/clip/clip_inference_request/","title":"CLIP Inference Request","text":""},{"location":"library/data_models/clip/clip_text_embedding_request/","title":"CLIP Text Embedding Request","text":""},{"location":"library/data_models/inference/classification/","title":"Classification Inference Request","text":""},{"location":"library/data_models/inference/cv_inference_request/","title":"CV Inference Request","text":""},{"location":"library/data_models/inference/inference_request_image/","title":"Inference Request Image","text":""},{"location":"library/data_models/inference/instance_segmentation/","title":"Instance Segmentation Inference Request","text":""},{"location":"library/data_models/inference/object_detection/","title":"Object Detection Inference Request","text":""},{"location":"library/data_models/sam/sam_embedding_request/","title":"SAM Embedding Request","text":""},{"location":"library/data_models/sam/sam_segmentation_request/","title":"SAM Segmentation Request","text":""},{"location":"library/interfaces/base/","title":"Base Interface","text":""},{"location":"library/interfaces/http/","title":"HTTP Interface","text":""},{"location":"library/model_managers/base/","title":"Base Model Manager","text":""},{"location":"library/model_managers/pingback/","title":"Inference Server Pingback Manager","text":""},{"location":"library/model_registries/base/","title":"Base Model Registry","text":""},{"location":"library/model_registries/roboflow/","title":"Roboflow Model Registry","text":""},{"location":"library/models/clip/","title":"CLIP","text":""},{"location":"library/models/vit_classification/","title":"ViT Classification","text":""},{"location":"library/models/yoloact_segmentation/","title":"YOLO-ACT Instance Segmentation","text":""},{"location":"library/models/yolov5/","title":"YOLOv5 Object Detection","text":""},{"location":"library/models/yolov5_segmentation/","title":"YOLOv5 Instance Segmentation","text":""},{"location":"library/models/yolov7_instance_segmentation/","title":"YOLOv7 Instance Segmentation","text":""},{"location":"library/models/yolov8/","title":"YOLOv8 Object Detection","text":""},{"location":"library/models/yolov8_classification/","title":"YOLOv8 Classification","text":""},{"location":"library/models/yolov8_segmentation/","title":"YOLOv8 Instance Segmentation","text":""},{"location":"quickstart/docker/","title":"Docker","text":""},{"location":"quickstart/docker/#setup","title":"Setup","text":"<p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment,  allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If  you haven't installed Docker yet, you can get it from Docker's official website.</p>"},{"location":"quickstart/docker/#pull","title":"Pull","text":"<p>If you don't wish to build the Docker image locally or prefer to use the official releases, you can directly pull the  pre-built images from the Docker Hub. These images are maintained by the Roboflow team and are optimized for various  hardware configurations.</p> <p>docker pull</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <p>Official Roboflow Inference Server Docker Image for x86 CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for ARM CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-arm-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-gpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU with TensorRT Runtime Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt-jetson\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-trt-jetson-5.1.1\n</code></pre>"},{"location":"quickstart/docker/#run","title":"Run","text":"<p>Once you have a Docker image (either built locally or pulled from Docker Hub), you can run the Roboflow Inference  Server in a container. </p> <p>docker run</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <pre><code>docker run --net=host \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run --net=host \\\nroboflow/roboflow-inference-server-arm-cpu:latest\n</code></pre> <pre><code>docker run --network=host --gpus=all \\\nroboflow/roboflow-inference-server-gpu:latest\n</code></pre> <pre><code>docker run --network=host --gpus=all \\\nroboflow/roboflow-inference-server-trt:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-trt-jetson:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-trt-jetson-5.1.1:latest\n</code></pre>"},{"location":"quickstart/docker/#build","title":"Build","text":"<p>To build a Docker image locally, first clone the Inference Server repository.</p> <pre><code>git clone git clone https://github.com/roboflow/inference\n</code></pre> <p>Choose a Dockerfile from the following options, depending on the hardware you want to run Inference Server on.</p> <p>docker build</p> x86 CPUarm64 CPUGPUGPU + TensorRTJetson 4.xJetson 5.x <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.arm.cpu \\\n-t roboflow/roboflow-inference-server-arm-cpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.gpu \\\n-t roboflow/roboflow-inference-server-gpu .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.trt \\\nroboflow/roboflow-inference-server-trt .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-trt-jetson .\n</code></pre> <pre><code>docker build \\\n-f dockerfiles/Dockerfile.onnx.jetson.5.1.1 \\\n-t roboflow/roboflow-inference-server-trt-jetson-5.1.1 .\n</code></pre>"},{"location":"quickstart/http_inference/","title":"HTTP Inference","text":"<p>The Roboflow Inference Server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we walk through how to run inference on object detection, classification, and segmentation models using the Inference Server.</p> <p>Currently, the server is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>To run inference with the server, we will:</p> <ol> <li>Install the server</li> <li>Download a model for use on the server</li> <li>Run inference</li> </ol>"},{"location":"quickstart/http_inference/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using. Here are the available Docker containers:</p>"},{"location":"quickstart/http_inference/#arm-cpu","title":"ARM CPU","text":"<pre><code>sudo docker run -it --rm -p 9001:9001 roboflow/roboflow-inference-server-arm-cpu\n</code></pre>"},{"location":"quickstart/http_inference/#trt","title":"TRT","text":"<pre><code>sudo docker run --privileged --net=host --runtime=nvidia --mount source=roboflow,target=/cache -e NUM_WORKERS=1 roboflow/roboflow-inference-server-trt-jetson:latest\n</code></pre>"},{"location":"quickstart/http_inference/#gpu","title":"GPU","text":"<pre><code>[]\n</code></pre> <p>Run the relevant command for your device. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p> <p>Now we are ready to run inference!</p>"},{"location":"quickstart/http_inference/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>To run inference on a model, we will make a HTTP request to:</p> <pre><code>http://localhost:9001/{workspace_id}/{model_id}\n</code></pre> <p>To find your workspace and model IDs, refer to the Roboflow documentation.</p> <p>This route works for all supported task types: object detection, classification, and segmentation.</p> <p>To run inference, make a HTTP request to the route:</p> <pre><code>import requests\n\nworkspace_id = \"\"\nmodel_id = \"\"\nimage_url = \"\"\nconfidence = 0.75\napi_key = \"\"\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n    json=infer_object_detection_payload,\n)\n\npredictions = res.json()\n</code></pre> <p>This code will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>Above, set your workspace and model ID. Also configure your confidence and IoU threshold values as needed. If you are using classification, you can omit the IoU threshold value. You will also need to set your Roboflow API key. To learn how to retrieve your Roboflow API key, refer to the Roboflow API documentation.</p> <p>You can post either a URL, a base64-encoded image, or a pickled NumPy array to the server.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/library_inference/","title":"Python Inference","text":"<p>You can run inference on models in the Roboflow Inference Server directly using the <code>inference</code> Python package. This allows you to interface with models without running the Inference HTTP server.</p> <p>In this guide, we will show how to run inference on object detection, classification, and segmentation models using the <code>inference</code> package.</p> <p>Let's begin!</p>"},{"location":"quickstart/library_inference/#step-1-install-roboflow-inference","title":"Step 1: Install Roboflow Inference","text":"<p>First, we need to install Roboflow Inference. The command to install Roboflow Inference depends on the device on whihc you are running inference. Here are the available packages:</p> <ul> <li><code>inference[arm]</code>: ARM CPU</li> <li><code>inference[jetson]</code>: NVIDIA Jetson</li> <li><code>inference[trt]</code>: TensorRT devices</li> </ul> <p>Run the relevant command on your device. Once you have installed the Python package, you can start running inference.</p>"},{"location":"quickstart/library_inference/#step-2-choose-a-model","title":"Step 2: Choose a Model","text":"<p>At the moment, you can only run inference on models trained on Roboflow. Support for bringing your own models is being actively worked on. This guide will be updated when such support is available.</p>"}]}